{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sin, cos, sqrt, atan2, atan, radians\n",
    "from itertools import compress\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "from astropy.io import ascii\n",
    "from os import listdir\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import shutil\n",
    "import time\n",
    "import math\n",
    "import gzip\n",
    "import json\n",
    "import os \n",
    "\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################\n",
    "def find_files( path_to_dir, suffix=\".txt\" ):\n",
    "    filenames = listdir(path_to_dir)\n",
    "    return [ filename for filename in filenames if filename.endswith( suffix ) ]\n",
    "\n",
    "##############################################################################################################\n",
    "#Snow/SWE observing thresholds\n",
    "Snowfall_TH    = 50.8 #mm\n",
    "SnowWaterEq_TH = 2.8 #mm\n",
    "\n",
    "#Anselin Morans I: Detect spatial outliers and remove them. Coords must bein lat/lon\n",
    "#Constants\n",
    "RadEarth   = 6371000 #m Euclidean Dist.\n",
    "\n",
    "def Outlier_Check(XYZ,dz,pwr):\n",
    "    dsize    = XYZ.shape\n",
    "    n        = int((dsize[0]))\n",
    "    print(\"All available observations pre-outlier analysis: \" + str(n))\n",
    "    #Pre-alo Variables\n",
    "    IDW      = np.zeros((n,n))\n",
    "    widw     = np.zeros((n,n))\n",
    "    hij      = np.zeros((n,n))\n",
    "    #Loop through each point\n",
    "    for i in range(n):\n",
    "        #Determine euclidean distance (km) from each observation [RadEarth*c/1000]\n",
    "        LonD        = np.radians(XYZ[:,0])\n",
    "        LatD        = np.radians(XYZ[:,1])\n",
    "        delta_phi   = LatD[i]-LatD\n",
    "        delta_lamda = LonD[i]-LonD\n",
    "        a           = np.sin(delta_phi/2)**2 + np.cos(LatD[i])*np.cos(LatD)*np.sin(delta_lamda/2.0)**2\n",
    "        a[a==0]     = np.nan\n",
    "        c           = 2*np.arctan(np.sqrt(a)/np.sqrt(1-a)) #getting errors in a\n",
    "        #Replace these values with nan... avoid error message\n",
    "        c[c==0]     = np.nan\n",
    "        #Determine Inverse Distance Weight between each observation\n",
    "        IDW[i,:]    = 1/(RadEarth*c/1000)**pwr\n",
    "        IDW[i,i]    = np.nan\n",
    "        #Index observations for each stations\n",
    "        widw[:,i]   = XYZ[i,2]\n",
    "        widw[i,i]   = np.nan\n",
    "        #Determine Euclidean distance b/n each station\n",
    "        hij[i,:]    = RadEarth*c/1000\n",
    "        hij[i,i]    = 99999\n",
    "    #Are the observations w/n threshold that has been defined\n",
    "    Eucl_Logical    = hij<dz #getting errors in hij\n",
    "    Sum_row_Eucl    = np.nansum(IDW*Eucl_Logical, axis=1)\n",
    "    #Weighted distance b/n observations as a ratio\n",
    "    Wij = np.full((n, n),np.nan)\n",
    "    for i in range(n):\n",
    "        if Sum_row_Eucl[i] > 0:\n",
    "            Wij[i,:] = np.round(IDW[i,:]/Sum_row_Eucl[i],2)        \n",
    "        Wij[i,i] = np.nan\n",
    "    #Observations with no cooresponding stations within a given threshold will have Wij == inf... \n",
    "    #Replace these values with nan\n",
    "    std_idw_p             = np.std(XYZ[:,2])\n",
    "    #widw_std = widw*Eucl_Logical\n",
    "    #widw_std[widw_std==0] = np.nan\n",
    "    #std_idw  = np.nanstd(widw_std,axis=1)\n",
    "    #std_idw[std_idw==0] = std_idw_p\n",
    "    std_idw           = np.std(XYZ[:,2])\n",
    "    #std_idw           = np.std(Wij*widw*Eucl_Logical,axis=1)\n",
    "    weighted_variable = np.nansum(Wij*widw*Eucl_Logical,axis=1)\n",
    "    #Z-score for a normal distribution\n",
    "    Zscore_i          = (XYZ[:,2]-weighted_variable)/std_idw\n",
    "    #non_outliers      = abs(Zscore_i)<1.645\n",
    "    #Z-score for a log distribution\n",
    "    #Zscore_i          = np.exp((XYZ[:,2]-weighted_variable)/std_idw)\n",
    "    Loggical_arraay   = [np.logical_and(Zscore_i > -1.280, Zscore_i < 100)] #v == 4\n",
    "\n",
    "    #Loggical_arraay   = [np.logical_and(Zscore_i > 0.711, Zscore_i < 9.488)] #v == 4\n",
    "    #Loggical_arraay   = [np.logical_and(Zscore_i > 2.733, Zscore_i < 15.507)] #v == 12\n",
    "    #Return outliers\n",
    "    XYZ_outliers      = XYZ[Loggical_arraay[0]==False]\n",
    "    #Return nonoutliers\n",
    "    XYZ_nonoutliers   = XYZ[Loggical_arraay[0]==True]\n",
    "    print(\"Outliers detected: \" + str(len(XYZ_outliers)))\n",
    "    print(\"Specifications: Interpolation == IDW to the \" + str(pwr) + \" power\")\n",
    "    print(\"Specifications: Search Radius == \" + str(dz) + \"km\")\n",
    "    print(\"----------------------------------------------\")\n",
    "    return(XYZ_nonoutliers,XYZ_outliers,Zscore_i)\n",
    "\n",
    "def Density_Check(lons,lats,data,dz,dp):\n",
    "    XYZ      = np.concatenate((np.expand_dims(lons,1), np.expand_dims(lats,1), np.expand_dims(data,1)), axis=1)\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(\"Initial number of observations: \" + str(len(XYZ)))\n",
    "    n        = len(XYZ)\n",
    "    #Pre-alo Variables\n",
    "    hij      = np.zeros((n,n))\n",
    "    #Loop through each point\n",
    "    for i in range(n):\n",
    "        #Determine euclidean distance (km) from each observation [RadEarth*c/1000]\n",
    "        LonD        = np.radians(XYZ[:,0])\n",
    "        LatD        = np.radians(XYZ[:,1])\n",
    "        delta_phi   = LatD[i]-LatD\n",
    "        delta_lamda = LonD[i]-LonD\n",
    "        a           = np.sin(delta_phi/2)**2 + np.cos(LatD[i])*np.cos(LatD)*np.sin(delta_lamda/2.0)**2\n",
    "        a[a==0]     = np.nan\n",
    "        c           = 2*np.arctan(np.sqrt(a)/np.sqrt(1-a)) #getting errors in a\n",
    "        #Replace these values with nan... avoid error message\n",
    "        c[c==0]     = np.nan\n",
    "        #Determine Euclidean distance b/n each station\n",
    "        hij[i,:]    = RadEarth*c/1000\n",
    "        hij[i,i]    = 99999\n",
    "    #Are the observations w/n threshold that has been defined\n",
    "    Eucl_Logical    = hij<dz #getting errors in hij\n",
    "    Eucl_Logical    = np.expand_dims(np.sum(Eucl_Logical,axis=1),1)\n",
    "    XYZ             = np.asarray(list(compress(XYZ,(Eucl_Logical>dp)==True)))\n",
    "    print(\"Clustered number of observations: \" + str(int(len(XYZ))))\n",
    "    print(\"Specifications: Distance == \" + str(dz) + \"km Point Density == \" + str(dp))\n",
    "    print(\"----------------------------------------------\")\n",
    "    return(XYZ)\n",
    "\n",
    "def Dumby_Check(lons, lats, query_lon, query_lat, dz):\n",
    "    hij        = np.zeros((len(lons),1))\n",
    "    for z in range(len(lons)):\n",
    "        # Constant\n",
    "        R = RadEarth\n",
    "        # Observation points\n",
    "        lat1 = lats[z]\n",
    "        lon1 = lons[z]\n",
    "        # Query/grid point\n",
    "        lat2 = query_lat\n",
    "        lon2 = query_lon\n",
    "        # Calculation\n",
    "        phi1, phi2 = math.radians(lat1), math.radians(lat2) \n",
    "        dphi       = math.radians(lat2 - lat1)\n",
    "        dlambda    = math.radians(lon2 - lon1)\n",
    "        a          = math.sin(dphi/2)**2 + math.cos(phi1)*math.cos(phi2)*math.sin(dlambda/2)**2\n",
    "        # Euclidean distance in km\n",
    "        hij[z,0]   = ((2*R*math.atan2(math.sqrt(a), math.sqrt(1 - a)))/1000)\n",
    "    hij            = np.sum(hij<=dz)\n",
    "    # Return the number of stations within dz kms of an observation.\n",
    "    return(hij)\n",
    "\n",
    "def Dumby_Check_02(XYZ, query_lon, query_lat, dz):\n",
    "    hij        = np.zeros((len(XYZ),1))\n",
    "    for z in range(len(XYZ)):\n",
    "        # Constant\n",
    "        R = RadEarth\n",
    "        # Observation points\n",
    "        lat1 = XYZ[z,1]\n",
    "        lon1 = XYZ[z,0]\n",
    "        # Query/grid point\n",
    "        lat2 = query_lat\n",
    "        lon2 = query_lon\n",
    "        # Calculation\n",
    "        phi1, phi2 = math.radians(lat1), math.radians(lat2) \n",
    "        dphi       = math.radians(lat2 - lat1)\n",
    "        dlambda    = math.radians(lon2 - lon1)\n",
    "        a          = math.sin(dphi/2)**2 + math.cos(phi1)*math.cos(phi2)*math.sin(dlambda/2)**2\n",
    "        # Euclidean distance in km\n",
    "        hij[z,0]   = ((2*R*math.atan2(math.sqrt(a), math.sqrt(1 - a)))/1000)\n",
    "    XYZ = XYZ[hij[:,0]<dz]\n",
    "    # Return the number of stations within dz kms of an observation.\n",
    "    return(XYZ)\n",
    "\n",
    "def simple_idw(x, y, z, xi, yi):\n",
    "    dist = distance_matrix(x,y, xi,yi)\n",
    "\n",
    "    # In IDW, weights are 1 / distance\n",
    "    weights = 1.0 / dist\n",
    "\n",
    "    # Make weights sum to one\n",
    "    weights /= weights.sum(axis=0)\n",
    "\n",
    "    # Multiply the weights for each interpolated point by all observed Z-values\n",
    "    zi = np.dot(weights.T, z)\n",
    "    return zi\n",
    "\n",
    "def distance_matrix(x0, y0, x1, y1):\n",
    "    obs = np.vstack((x0, y0)).T\n",
    "    interp = np.vstack((x1, y1)).T\n",
    "\n",
    "    # Make a distance matrix between pairwise observations\n",
    "    # Note: from <http://stackoverflow.com/questions/1871536>\n",
    "    # (Yay for ufuncs!)\n",
    "    d0 = np.subtract.outer(obs[:,0], interp[:,0])\n",
    "    d1 = np.subtract.outer(obs[:,1], interp[:,1])\n",
    "\n",
    "    return np.hypot(d0, d1)\n",
    "\n",
    "def YYYYMMDD_string(Datetime_Value):\n",
    "    if Datetime_Value.month>9: Month_str = str(Datetime_Value.month)\n",
    "    else: Month_str = \"0\"+str(Datetime_Value.month)   \n",
    "    if Datetime_Value.day>9: Day_str = str(Datetime_Value.day)\n",
    "    else: Day_str = \"0\"+str(Datetime_Value.day)\n",
    "    return (str(Datetime_Value.year)+Month_str+Day_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(event_start)):\n",
    "    if event_start[i][2] != event_end[i][2]:\n",
    "        print(event_start[i])\n",
    "        print(event_end[i])\n",
    "        \n",
    "event_start = [[12,30,2013],[12,28,2009],[12,28,2000],[12,31,1970],[12,31,1924]]\n",
    "event_end   = [[1,3,2014]  ,[1,4,2010]  ,[1,1,2001]  ,[1,2,1971]  ,[1,3,1925]  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_start = [[10,29,2020],[11,29,2019],[1,18,2019],[11,14,2018],[3,20,2018],[3,11,2018],[3,5,2018],[3,1,2018],[1,3,2018],\n",
    "                [3,12,2017],[2,9,2017],[11,17,2016],[1,22,2016],[2,14,2015],[2,8,2015],[1,29,2015],[1,25,2015],\n",
    "                [12,9,2014],[11,26,2014],[2,11,2014],[1,20,2014],[12,30,2013],[12,13,2013],[3,17,2013],[3,3,2013],\n",
    "                [2,8,2013],[12,28,2012],[12,24,2012],[10,25,2011],[2,24,2011],[2,1,2011],[1,26,2011],[1,9,2011],\n",
    "                [12,24,2010],[2,21,2010],[2,12,2010],[2,8,2010],[2,4,2010],[12,28,2009],[12,18,2009],[12,7,2009],\n",
    "                [2,26,2009],[2,22,2009],[12,21,2008],[12,18,2008],[12,14,2007],[11,30,2007],[4,3,2007],[3,16,2007],\n",
    "                [2,11,2007],[2,10,2006],[2,28,2005],[1,22,2005],[12,4,2003],[2,14,2003],[1,1,2003],[12,23,2002],[12,28,2000],[2,16,2000],[1,24,2000],[1,24,2000],\n",
    "                [3,12,1999],[1,13,1999],[3,31,1997],[1,8,1997],[4,9,1996],[3,3,1996],[2,1,1996],[1,6,1996],[12,18,1995],\n",
    "                [2,2,1995],[2,28,1994],[2,22,1994],[1,16,1994],[1,4,1994],[1,1,1994],[3,12,1993],[2,20,1993],[2,14,1993],\n",
    "                [12,9,1992],[12,27,1990],[2,9,1988],[1,22,1988],[1,5,1988],[12,13,1987],[11,10,1987],[2,22,1987],[1,21,1987],\n",
    "                [1,8,1987],[1,1,1987],[3,1,1985],[1,29,1985],[2,24,1984],[2,10,1983],[4,4,1982],[1,20,1982],[1,12,1982],\n",
    "                [2,17,1979],[2,6,1979],[2,4,1978],[1,17,1978],[1,14,1978],[1,11,1978],[3,21,1977],[1,7,1977],[1,8,1974],\n",
    "                [12,15,1973],[2,16,1972],[11,23,1971],[2,26,1971],[12,31,1970],[12,25,1969],[2,25,1969],[2,22,1969],[2,8,1969],\n",
    "                [3,20,1967],[2,6,1967],[12,22,1966],[2,23,1966],[1,28,1966],[1,21,1966],[2,18,1964],[1,9,1964],[12,21,1963],\n",
    "                [3,5,1962],[2,28,1962],[2,13,1962],[2,1,1961],[1,18,1961],[12,10,1960],[2,29,1960],[2,12,1960],[3,12,1959],\n",
    "                [3,18,1958],[2,12,1958],[12,3,1957],[3,18,1956],[3,14,1956],[3,3,1956],[2,17,1952],[12,13,1951],[11,22,1950],\n",
    "                [2,11,1950],[1,29,1949],[1,23,1948],[12,25,1947],[2,27,1947],[2,20,1947],[2,15,1946],[12,17,1945],[1,13,1945],\n",
    "                [12,8,1944],[2,8,1944],[1,25,1943],[3,28,1942],[2,28,1942],[3,7,1941],[2,13,1940],[11,23,1938],[2,10,1936],\n",
    "                [1,18,1936],[1,21,1935],[2,23,1934],[12,26,1933],[12,16,1932],[3,4,1931],[12,19,1929],[2,20,1929],[2,16,1927],\n",
    "                [2,3,1926],[1,7,1926],[1,28,1925],[12,31,1924],[4,1,1924],[2,17,1924],[2,3,1923],[1,26,1922],[2,18,1921],[2,4,1920],\n",
    "                [1,25,1918],[1,21,1918],[1,12,1918],[12,12,1917],[12,6,1917],[3,2,1917],[3,2,1916],[12,10,1915],[4,3,1915],[3,2,1915],\n",
    "                [1,29,1915],[2,12,1914],[2,16,1910],[2,10,1910],[1,12,1910],[12,23,1909],[3,2,1909],[1,27,1909],[1,10,1909],[2,16,1908],\n",
    "                [2,3,1908],[1,29,1908],[2,4,1907],[3,17,1906],[3,12,1906],[1,27,1904],[2,14,1903],[12,11,1902],[3,3,1902],[2,13,1902],\n",
    "                [2,1,1901],[3,15,1900],[2,26,1900]]\n",
    "event_end   = [[10,30,2020],[12,3,2019],[1,21,2019],[11,16,2018],[3,22,2018],[3,15,2018],[3,8,2018],[3,3,2018],[1,5,2018],\n",
    "                [3,15,2017],[2,10,2017],[11,22,2016],[1,24,2016],[2,16,2015],[2,10,2015],[2,3,2015],[1,28,2015],\n",
    "                [12,14,2014],[11,28,2014],[2,14,2014],[1,22,2014],[1,3,2014],[12,16,2013],[3,20,2013],[3,9,2013],\n",
    "                [2,10,2013],[12,31,2012],[12,28,2012],[10,31,2011],[2,27,2011],[2,4,2011],[1,27,2011],[1,13,2011],\n",
    "                [12,28,2010],[3,1,2010],[2,19,2010],[2,11,2010],[2,8,2010],[1,4,2010],[12,21,2009],[12,11,2009],\n",
    "                [3,3,2009],[2,24,2009],[12,23,2008],[12,22,2008],[12,17,2007],[12,4,2007],[4,6,2007],[3,18,2007],\n",
    "                [2,16,2007],[2,14,2006],[3,2,2005],[1,24,2005], [12,8,2003],[2,18,2003],[1,4,2003],[12,26,2002],[1,1,2001],[2,20,2000],[2,1,2000],[1,27,2000],\n",
    "                [3,16,1999],[1,16,1999],[4,1,1997],[1,12,1997],[4,11,1996],[3,9,1996],[2,4,1996],[1,9,1996],\n",
    "                [12,22,1995],[2,6,1995],[3,4,1994],[2,25,1994],[1,18,1994],[1,9,1994],[1,5,1994],[3,15,1993],\n",
    "                [2,24,1993],[2,18,1993],[12,13,1992],[12,29,1990],[2,14,1988],[1,27,1988],[1,9,1988],[12,17,1987],\n",
    "                [11,12,1987],[2,24,1987],[1,24,1987],[1,12,1987],[1,3,1987],[3,5,1985],[2,3,1985],[3,1,1984],\n",
    "                [2,13,1983],[4,8,1982],[1,24,1982],[1,15,1982],[2,20,1979],[2,9,1979],[2,8,1978],[1,21,1978],\n",
    "                [1,19,1978],[1,15,1978],[3,25,1977],[1,11,1977],[1,12,1974],[12,18,1973],[2,20,1972],[11,27,1971],\n",
    "                [3,6,1971],[1,2,1971],[12,29,1969],[3,4,1969],[2,28,1969],[2,10,1969],[3,23,1967],[2,8,1967],\n",
    "                [12,26,1966],[2,26,1966],[2,1,1966],[1,24,1966],[2,21,1964],[1,14,1964],[12,24,1963],[3,9,1962],\n",
    "                [3,7,1962],[2,16,1962],[2,5,1961],[1,21,1961],[12,13,1960],[3,5,1960],[2,15,1960],[3,14,1959],\n",
    "                [3,23,1958],[2,18,1958],[12,5,1957],[3,20,1956],[3,17,1956],[3,9,1956],[2,18,1952],[12,16,1951],\n",
    "                [11,30,1950],[2,17,1950],[2,1,1949],[1,25,1948],[12,27,1947],[3,4,1947],[2,24,1947],[2,21,1946],\n",
    "                [12,20,1945],[1,17,1945],[12,13,1944],[2,13,1944],[1,29,1943],[3,31,1942],[3,4,1942],[3,10,1941],\n",
    "                [2,15,1940],[11,25,1938],[2,15,1936],[1,21,1936],[1,25,1935],[2,27,1934],[12,27,1933],[12,18,1932],\n",
    "                [3,12,1931],[12,24,1929],[2,22,1929],[2,21,1927],[2,5,1926],[1,10,1926],[1,31,1925],[1,3,1925],\n",
    "                [4,4,1924],[2,21,1924],[2,7,1923],[1,30,1922],[2,22,1921],[2,7,1920],[1,29,1918],[1,23,1918],\n",
    "                [1,16,1918],[12,15,1917],[12,9,1917],[3,6,1917],[3,9,1916],[12,15,1915],[4,5,1915],[3,8,1915],\n",
    "                [2,3,1915],[2,15,1914],[2,18,1910],[2,13,1910],[1,15,1910],[12,27,1909],[3,5,1909],[1,31,1909],\n",
    "                [1,15,1909],[2,20,1908],[2,7,1908],[2,2,1908],[2,6,1907],[3,20,1906],[3,17,1906],[1,30,1904],\n",
    "                [2,18,1903],[12,14,1902],[3,6,1902],[2,19,1902],[2,6,1901],[3,16,1900],[3,3,1900]]\n",
    "print(len(event_start),len(event_end))\n",
    "#print(event_start[20],event_end[20])\n",
    "#event_start = list(event_start[20])\n",
    "#event_end   = list(event_end[20])\n",
    "#add the access token you got from NOAA\n",
    "Variable      = 'SNOW' #SNOW,PRCP,TAVG \n",
    "Parent_dir    = 'C:/Users/Mike/NESIS_Kriging/'\n",
    "# Overwrite file if date isn't available in previous csv file\n",
    "Overwrite  =  0 #1==yes 0==no\n",
    "# Analysis Domain\n",
    "North      = 55.0\n",
    "East       = -60.0\n",
    "South      = 25.0\n",
    "West       = -110.0\n",
    "# Dumby Grid\n",
    "grid_space     = 0.25\n",
    "grid_lon       = np.arange(West , East , grid_space)\n",
    "grid_lat       = np.arange(South, North, grid_space) #grid_space is the desired delta/step of the output array \n",
    "xintrp, yintrp = np.meshgrid(grid_lon, grid_lat)\n",
    "#############################################################################################################\n",
    "# Retrieve station metadata if not obtained\n",
    "GHCND_Base_FTP   = 'ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/'\n",
    "Station_File     = 'ghcnd-stations.txt'\n",
    "Archive_Dir      = 'C:/Users/Mike/Desktop/GHCND/AirMG/'\n",
    "List_Archive_Dir = find_files( Archive_Dir, suffix=\".txt\" )\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "if Station_File not in List_Archive_Dir:\n",
    "    print(\"Downloading \"+Station_File)\n",
    "    Station_File_URL = GHCND_Base_FTP + Station_File\n",
    "    urllib.request.urlretrieve(Station_File_URL, Station_File)\n",
    "    shutil.move(Station_File,Archive_Dir)\n",
    "else: print(Station_File+\" is Downloaded: \"+Archive_Dir)\n",
    "#############################################################################################################\n",
    "# Parse the station metadata\n",
    "Station_File         = 'ghcnd-stations.txt'\n",
    "with open(Archive_Dir+Station_File) as f:\n",
    "    Station_Metadata = f.readlines()\n",
    "Station_Metadata     = [x.strip() for x in Station_Metadata] \n",
    "Station_Metadata_np  = np.empty([len(Station_Metadata),4], dtype=object)\n",
    "for x in range(len(Station_Metadata)):\n",
    "    query                    = Station_Metadata[x]\n",
    "    Station_Metadata_np[x,0] = query[0:11].strip()  #Station I D\n",
    "    Station_Metadata_np[x,1] = query[12:20].strip() #Lat\n",
    "    Station_Metadata_np[x,2] = query[21:30].strip() #Lon\n",
    "    # Determine which stations fit within the analysis domain\n",
    "    if (float(query[12:20].strip()) < North) and (float(query[12:20].strip()) > South) and \\\n",
    "       (float(query[21:30].strip()) < East) and (float(query[21:30].strip()) > West): Station_Metadata_np[x,3] = '1'\n",
    "    else: Station_Metadata_np[x,3] = '0'\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "print(Station_File+\" file has been parsed!\")\n",
    "#Column headers for spark dataframe\n",
    "schema = StructType([\n",
    "        StructField(\"Station\", StringType()),\n",
    "        StructField(\"YYYYMMDD\", StringType()),\n",
    "        StructField(\"Type\", StringType()),\n",
    "        StructField(\"VALUE\", DoubleType()),\n",
    "        StructField(\"MFLAG1\", StringType()),\n",
    "        StructField(\"QFLAG1\", StringType()),\n",
    "        StructField(\"SFLAG1\", StringType()),\n",
    "        StructField(\"ObsTime\", StringType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################\n",
    "#############################################################################################################\n",
    "#############################################################################################################\n",
    "for event in range(len(event_end)):\n",
    "    start_time = time.time()\n",
    "    # Window of analysis\n",
    "    D_Start   = datetime(event_start[event][2],event_start[event][0],event_start[event][1]).date()\n",
    "    D_End     = datetime(event_end[event][2],event_end[event][0],event_end[event][1]).date()\n",
    "    CSV_PRINT = YYYYMMDD_string(D_Start) + \"_\" + YYYYMMDD_string(D_End) + \"_\" + Variable + \".csv\"\n",
    "    SHP_PRINT = YYYYMMDD_string(D_Start) + \"_\" + YYYYMMDD_string(D_End) + \"_\" + Variable + \".shp\" \n",
    "    DIR_PRINT = YYYYMMDD_string(D_Start) + \"_\" + YYYYMMDD_string(D_End) \n",
    "    # Creating event director for .shp and .csv files \n",
    "    Path      = os.path.join(Parent_dir, DIR_PRINT) \n",
    "    try: os.mkdir(Path) \n",
    "    except: pass\n",
    "    # \n",
    "    Dates     = []\n",
    "    D         = D_End-D_Start\n",
    "    D         = D.days+1\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(\"Summing Precip. for: \")\n",
    "    for x in range(D):\n",
    "        Date_Keep = D_Start+timedelta(days=x)\n",
    "        Dates.append(Date_Keep) \n",
    "        print(Date_Keep)\n",
    "    print(\"------------------------------------------------------------------------\")   \n",
    "    Year_list = []\n",
    "    for days in Dates:\n",
    "        Year_list.append(days.year)\n",
    "    Year_list = np.unique(Year_list)\n",
    "    print(\"Need yearly .csv files for: \") \n",
    "    print(Year_list[:])\n",
    "    #############################################################################################################\n",
    "    # Repeat for yearly GHCND data\n",
    "    List_Archive_Dir = find_files( Archive_Dir, suffix=\".csv\" )\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    for year in Year_list:\n",
    "        csv_file = str(year)+'.csv'\n",
    "        if csv_file not in List_Archive_Dir or Overwrite==1:\n",
    "            print(\"Downloading \"+csv_file+'.gz')\n",
    "            Station_File_URL = GHCND_Base_FTP + 'by_year/' + csv_file + '.gz'\n",
    "            urllib.request.urlretrieve(Station_File_URL, csv_file + '.gz')\n",
    "            shutil.move(csv_file + '.gz',Archive_Dir)\n",
    "            with gzip.open(Archive_Dir+csv_file+'.gz', 'rb') as f_in:\n",
    "                with open(Archive_Dir+csv_file, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "        else: print(csv_file+\" is Downloaded: \"+Archive_Dir)\n",
    "    #############################################################################################################\n",
    "    # Parse the station metadata\n",
    "    #Station_File         = 'ghcnd-inventory.txt'\n",
    "    #Station_Inventory    = pd.read_csv(Archive_Dir+Station_File, header=None, delimiter=r\"\\s+\")\n",
    "    #Station_Inventory.rename(columns={0:'GHCND-ID',1:'Lat',2:'Lon',3:'Variable',4:'YYYY_Start',5:'YYYY_End'}, inplace=True)\n",
    "    #Station_Inventory    = Station_Inventory[Station_Inventory['Variable']==Variable].reset_index(drop=True)\n",
    "    #Station_Inventory    = Station_Inventory[(Station_Inventory['YYYY_Start']<=D_Start.year) & \\\n",
    "    #                                         (Station_Inventory['YYYY_End']>=D_End.year)].reset_index(drop=True)\n",
    "    #print(\"------------------------------------------------------------------------\")\n",
    "    #print(Station_File+\" file has been parsed!\")\n",
    "    #############################################################################################################\n",
    "    # Turn into dataframe\n",
    "    Station_Metadata_df = pd.DataFrame(Station_Metadata_np)\n",
    "    Station_Metadata_df.rename(columns={0:'GHCND-ID',1:'Lat',2:'Lon',3:'Boolean'}, inplace=True)\n",
    "    Station_Metadata_df = Station_Metadata_df[Station_Metadata_df['Boolean']=='1'].reset_index(drop=True)\n",
    "    #Station_Metadata_df = Station_Metadata_df.merge(Station_Inventory, on='GHCND-ID', how='inner').reset_index(drop=True)\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(\"There are \"+str(len(Station_Metadata_df))+\" stations within the analysis\")\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    #############################################################################################################\n",
    "    #############################################################################################################\n",
    "    #############################################################################################################\n",
    "\n",
    "    #Load massive csv file as a spark dataframe\n",
    "    if len(Year_list)>1:\n",
    "        Yearly_metadata_00 = spark.read.csv(Archive_Dir+str(Year_list[0])+'.csv', header = 'true', schema=schema)\n",
    "        Yearly_metadata_01 = spark.read.csv(Archive_Dir+str(Year_list[1])+'.csv', header = 'true', schema=schema)\n",
    "        Yearly_metadata    = Yearly_metadata_00.union(Yearly_metadata_01)\n",
    "    else: Yearly_metadata = spark.read.csv(Archive_Dir+csv_file, header = 'true', schema=schema)\n",
    "    print(\"1) Pyspark dataframe loaded...\")\n",
    "    #FILTER DATA TO MAKE IT MORE MANAGABLE\n",
    "    Type_drop       = ['TAVG','SNWD','TMAX','TMIN','ACMC','ACMH','ACSC','ACSH','AWND','DAEV','DAPR','DASF','DATN',\n",
    "                       'DATX','DAWM','DWPR','EVAP','FMTM','FRGB','FRGT','FRTH','GAHT','MDEV','MDPR','MDSF','MDTN',\n",
    "                       'MDTX','MDWM','MNPN','MXPN','PGTM','PSUN','SN*#','SX*#','THIC','TOBS','TSUN','WDF1','WDF2',\n",
    "                       'WDF5','WDFG','WDFI','WDFM','WDMV','WESD','WSF1','WSF2','WSF5','WSFG','WSFI','WSFM','WT**',\n",
    "                       'WVxx','WT01','WT02','WT11','WT13'] #Unuseful data\n",
    "    Quality_drop    = [\"D\",\"G\",\"I\",\"K\",\"L\",\"M\",\"N\",\"O\",\"R\",\"S\",\"T\",\"W\",\"X\",\"Z\"] #Fauly flags\n",
    "    Dates_drop      = []\n",
    "    for days in Dates:\n",
    "        YYYY_str    = str(days.year)\n",
    "        if days.month>9: MM_str = str(days.month)\n",
    "        else:            MM_str = '0'+str(days.month)\n",
    "        if days.day>9:   DD_str = str(days.day)\n",
    "        else:            DD_str = '0'+str(days.day)\n",
    "        Dates_drop.append(YYYY_str+MM_str+DD_str)\n",
    "    #REMOVING OBSERVATIONS\n",
    "    Yearly_metadata = Yearly_metadata[~Yearly_metadata['Type'].isin(Type_drop)] #REMOVE UNWANTED OBS\n",
    "    Yearly_metadata = Yearly_metadata[Yearly_metadata['YYYYMMDD'].isin(Dates_drop)] #REMOVE DATES THAT AREN'T IN TIME-WINDOW\n",
    "    Yearly_metadata = Yearly_metadata[Yearly_metadata['VALUE'] != 0]  #REMOVE 0 OBS\n",
    "    #STATIONS W/FLAGGED DAILY TOTALS\n",
    "    Flagged_Tots    = Yearly_metadata[~Yearly_metadata['QFLAG1'].isin(Quality_drop)==False]\n",
    "    Flagged_Stat    = Flagged_Tots.select(\"Station\").distinct()\n",
    "    Flagged_array   = [row.Station for row in Flagged_Stat.collect()]\n",
    "    #ROMVING SUSPECT STATIONS\n",
    "    Yearly_metadata = Yearly_metadata[~Yearly_metadata['Station'].isin(Flagged_array)]\n",
    "    print(\"2) Pyspark dataframe cleaned...\")\n",
    "    #SEPERATING DATA BY WEATHER OBSERVATION\n",
    "    Snowfall             = Yearly_metadata[Yearly_metadata['Type']=='SNOW']\n",
    "    #SnowWaterEq          = Yearly_metadata[Yearly_metadata['Type']=='WESF']\n",
    "    #LiquidWaterEq        = Yearly_metadata[Yearly_metadata['Type']=='PRCP']\n",
    "    print(\"3) Snowfall, SWE, and LWE dataframes created...\")\n",
    "    #SUM BY STATION\n",
    "    Snowfall_amnt        = Snowfall.groupBy('Station').sum()\n",
    "    Snowfall_amnt        = Snowfall_amnt[Snowfall_amnt['sum(VALUE)']>Snowfall_TH].select(\"*\").toPandas()\n",
    "    #SnowWaterEq_amnt     = SnowWaterEq.groupBy('Station').sum()\n",
    "    #SnowWaterEq_amnt     = SnowWaterEq_amnt[SnowWaterEq_amnt['sum(VALUE)']>SnowWaterEq_TH].select(\"*\").toPandas()\n",
    "    #LiquidWaterEq_amnt   = LiquidWaterEq.groupBy('Station').sum()\n",
    "    #LiquidWaterEq_amnt   = LiquidWaterEq_amnt[LiquidWaterEq_amnt['sum(VALUE)']>SnowWaterEq_TH].select(\"*\").toPandas()\n",
    "    print(\"4) Snowfall, SWE, and LWE dataframes summed by dates...\")\n",
    "    #JOIN COORDINATES \n",
    "    Snowfall_amnt        = pd.merge(Station_Metadata_df, Snowfall_amnt, left_on='GHCND-ID',right_on='Station',how='inner') #JOIN COORDS\n",
    "    #SnowWaterEq_amnt     = pd.merge(Station_Metadata_df, SnowWaterEq_amnt, left_on='GHCND-ID',right_on='Station',how='inner') #JOIN COORDS\n",
    "    #LiquidWaterEq_amnt   = pd.merge(Station_Metadata_df, LiquidWaterEq_amnt, left_on='GHCND-ID',right_on='Station',how='inner') #JOIN COORDS\n",
    "    print(\"5) Snowfall, SWE, and LWE dataframes joined by station...\")\n",
    "\n",
    "    #############################################################################################################\n",
    "    #############################################################################################################\n",
    "    #############################################################################################################\n",
    "    lons = np.array(Snowfall_amnt['Lon']).astype(float)\n",
    "    lats = np.array(Snowfall_amnt['Lat']).astype(float)\n",
    "    data = np.array(Snowfall_amnt['sum(VALUE)']).astype(float)*0.0393701\n",
    "    \n",
    "    save_data     = np.concatenate((np.expand_dims(np.asarray(lons),1), np.expand_dims(np.asarray(lats),1), \n",
    "                                     np.expand_dims(np.asarray(data),1)),axis=1)\n",
    "    save_data     = pd.DataFrame(save_data)\n",
    "    save_data.to_csv(\"complete_list\"+\"_\"+CSV_PRINT, sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "    from matplotlib.backends.backend_pdf import PdfPages\n",
    "    from matplotlib.patches import Path, PathPatch\n",
    "    from matplotlib.colors import BoundaryNorm\n",
    "    from cartopy.io.shapereader import Reader\n",
    "    from pykrige.ok import OrdinaryKriging\n",
    "    import matplotlib.colors as mcolors\n",
    "    from matplotlib.lines import Line2D\n",
    "    import matplotlib.ticker as mticker\n",
    "    import cartopy.feature as cfeature\n",
    "    import pykrige.kriging_tools as kt\n",
    "    import matplotlib.pyplot as plt\n",
    "    from itertools import compress\n",
    "    import cartopy.crs as ccrs\n",
    "    import cartopy.crs as ccrs\n",
    "    import matplotlib as mpl\n",
    "    from os import listdir\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import math\n",
    "    import os\n",
    "    # Colorbar function -------------------------------------------------------------------------------\n",
    "    def make_colorbar(ax, mappable, **kwargs):\n",
    "        divider = make_axes_locatable(ax)\n",
    "        orientation = kwargs.pop('orientation', 'vertical')\n",
    "        if orientation == 'vertical':\n",
    "            loc = 'right'\n",
    "        elif orientation == 'horizontal':\n",
    "            loc = 'bottom'\n",
    "        cax = divider.append_axes(loc, '5%', pad='3%', axes_class=mpl.pyplot.Axes)\n",
    "        ax.get_figure().colorbar(mappable, cax=cax, orientation=orientation,label=\"inch\", \n",
    "                                     ticks=[0,0.1,1,2,3,4,6,8,12,18,24,30,36,48,60,72,96])\n",
    "    # Creating colorbar\n",
    "    clevs     = [0,0.1,1,2,3,4,6,8,12,18,24,30,36,48,60,72,96]\n",
    "    cmap_data = [(0.96456693, 0.96456693, 0.96456693),\n",
    "                     (0.77952756, 0.86614173, 0.92125984),\n",
    "                     (0.51574803, 0.73228346, 0.86220472),\n",
    "                     (0.32283465, 0.58661417, 0.77952756),\n",
    "                     (0.18897638, 0.42913386, 0.66535433),\n",
    "                     (0.17716535, 0.28740157, 0.63385827),\n",
    "                     (1.        , 1.        , 0.6496063 ),\n",
    "                     (1.        , 0.80314961, 0.16535433),\n",
    "                     (1.        , 0.6023622 , 0.16141732),\n",
    "                     (0.88188976, 0.21259843, 0.15748031),\n",
    "                     (0.68503937, 0.15354331, 0.16929134),\n",
    "                     (0.50787402, 0.16535433, 0.16535433),\n",
    "                     (0.33858268, 0.16535433, 0.16535433),\n",
    "                     (0.83070866, 0.83070866, 1.        ),\n",
    "                     (0.68110236, 0.62204724, 0.87007874),\n",
    "                     (0.57086614, 0.43307087, 0.7007874 )]\n",
    "    cmap_snow      = mcolors.ListedColormap(cmap_data, 'precipitation')\n",
    "    norm_snow      = mcolors.BoundaryNorm(clevs, cmap_snow.N)\n",
    "    # Figure\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    # Map Resources\n",
    "    ax  = plt.subplot(1,1,1, projection=ccrs.Mercator())\n",
    "    ax.set_title('Historical Snowfall Interpolation: '+'\\n'+'title_str', fontsize=24, fontweight='bold')\n",
    "    ax.set_extent([West, East, South, North])\n",
    "    #ax.add_geometries(Reader(fname_can).geometries(), ccrs.PlateCarree(), facecolor=\"w\", hatch='\\\\\\\\\\\\\\\\',  edgecolor='black', lw=0.7)\n",
    "    #ax.add_geometries(Reader(fname_mask).geometries(), ccrs.PlateCarree(), facecolor=\"w\",  edgecolor='none')\n",
    "    # Non-outliers\n",
    "    lon  = XYZ_df[(XYZ_df['Boolean']==1) & (XYZ_df['data']>0)]['Lon']\n",
    "    lat  = XYZ_df[(XYZ_df['Boolean']==1) & (XYZ_df['data']>0)]['Lat']\n",
    "    data = XYZ_df[(XYZ_df['Boolean']==1) & (XYZ_df['data']>0)]['data']\n",
    "    cb  = plt.scatter(lon, lat,\n",
    "                     c=data, s=10, cmap=cmap_snow, norm=norm_snow,\n",
    "                     alpha=1, edgecolors='k', linewidths=0.5,\n",
    "                         zorder=4,marker='o', transform=ccrs.PlateCarree())\n",
    "    # Outliers\n",
    "    lon  = XYZ_df[(XYZ_df['Boolean']==0) & (XYZ_df['data']>0)]['Lon']\n",
    "    lat  = XYZ_df[(XYZ_df['Boolean']==0) & (XYZ_df['data']>0)]['Lat']\n",
    "    data = XYZ_df[(XYZ_df['Boolean']==0) & (XYZ_df['data']>0)]['data']\n",
    "    cb  = plt.scatter(lon, lat,\n",
    "                     c=data, s=15, cmap=cmap_snow, norm=norm_snow,\n",
    "                     alpha=1, edgecolors='k', linewidths=0.5,\n",
    "                         zorder=5,marker='X', transform=ccrs.PlateCarree())\n",
    "    \n",
    "    # Dumby Points\n",
    "    lon  = XYZ_df[(XYZ_df['Boolean']==1) & (XYZ_df['data']==0)]['Lon']\n",
    "    lat  = XYZ_df[(XYZ_df['Boolean']==1) & (XYZ_df['data']==0)]['Lat']\n",
    "    plt.scatter(lon, lat, s=5,\n",
    "                     alpha=1,facecolor='k', edgecolors='k', linewidths=0.5,\n",
    "                         zorder=4,marker='o', transform=ccrs.PlateCarree())\n",
    "    \n",
    "    ax.add_feature(cfeature.STATES, facecolor=\"none\", edgecolor='black', lw=0.5)\n",
    "    ax.add_feature(cfeature.LAKES, facecolor=\"lightcyan\", edgecolor='black', lw=0.5)\n",
    "    # Colorbar\n",
    "    make_colorbar(ax, cb, pad=0)\n",
    "    pp = PdfPages('Testing.pdf')\n",
    "    pp.savefig(fig)\n",
    "    pp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from matplotlib.patches import Path, PathPatch\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "from cartopy.io.shapereader import Reader\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.ticker as mticker\n",
    "import cartopy.feature as cfeature\n",
    "import pykrige.kriging_tools as kt\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import compress\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib as mpl\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "\n",
    "#####################################################################################################################\n",
    "### SETTING CONSTANTS\n",
    "#####################################################################################################################\n",
    "# Plotting window\n",
    "West_NE    = -82\n",
    "East_NE    = -66\n",
    "South_NE   = 37\n",
    "North_NE   = 48\n",
    "dxdy       = 0.5\n",
    "dxkrig     = 0.1\n",
    "Buffer_E   = 2\n",
    "Buffer_W   = 10\n",
    "Buffer_S   = 10\n",
    "Buffer_N   = 2 \n",
    "\n",
    "# Shapefiles for cartopy\n",
    "shp_path      = 'C:/Users/Mike/Desktop/Graduate_Work/Evaluation_Procedure/NWP_Domains_shp/Boundaries_Shapefiles/'\n",
    "fname_usa     = shp_path+'United_States_States_5m/cb_2016_us_state_5m'\n",
    "fname_can     = shp_path+'Canada_trsmd/canada_tr'\n",
    "fname_usa_cnt = shp_path+'/cb_2018_us_county_5m/cb_2018_us_county_5m.shp'\n",
    "fname_neas_cl = shp_path+'/Northeast_Selection/Northeast_Outline.shp'\n",
    "        \n",
    "# Creating colorbar\n",
    "clevs     = [0,0.1,1,2,3,4,6,8,12,18,24,30,36,48,60,72,96]\n",
    "cmap_data = [(0.96456693, 0.96456693, 0.96456693),\n",
    "                 (0.77952756, 0.86614173, 0.92125984),\n",
    "                 (0.51574803, 0.73228346, 0.86220472),\n",
    "                 (0.32283465, 0.58661417, 0.77952756),\n",
    "                 (0.18897638, 0.42913386, 0.66535433),\n",
    "                 (0.17716535, 0.28740157, 0.63385827),\n",
    "                 (1.        , 1.        , 0.6496063 ),\n",
    "                 (1.        , 0.80314961, 0.16535433),\n",
    "                 (1.        , 0.6023622 , 0.16141732),\n",
    "                 (0.88188976, 0.21259843, 0.15748031),\n",
    "                 (0.68503937, 0.15354331, 0.16929134),\n",
    "                 (0.50787402, 0.16535433, 0.16535433),\n",
    "                 (0.33858268, 0.16535433, 0.16535433),\n",
    "                 (0.83070866, 0.83070866, 1.        ),\n",
    "                 (0.68110236, 0.62204724, 0.87007874),\n",
    "                 (0.57086614, 0.43307087, 0.7007874 )]\n",
    "cmap_snow      = mcolors.ListedColormap(cmap_data, 'precipitation')\n",
    "norm_snow      = mcolors.BoundaryNorm(clevs, cmap_snow.N)\n",
    "#####################################################################################################################\n",
    "### DEFINING FUNCTIONS\n",
    "#####################################################################################################################\n",
    "# Colorbar function -------------------------------------------------------------------------------\n",
    "def make_colorbar(ax, mappable, **kwargs):\n",
    "    divider = make_axes_locatable(ax)\n",
    "    orientation = kwargs.pop('orientation', 'vertical')\n",
    "    if orientation == 'vertical':\n",
    "        loc = 'right'\n",
    "    elif orientation == 'horizontal':\n",
    "        loc = 'bottom'\n",
    "    cax = divider.append_axes(loc, '5%', pad='3%', axes_class=mpl.pyplot.Axes)\n",
    "    ax.get_figure().colorbar(mappable, cax=cax, orientation=orientation,label=\"inch\", \n",
    "                                 ticks=[0,0.1,1,2,3,4,6,8,12,18,24,30,36,48,60,72,96])\n",
    "# Spatial Outlier Algorithm -------------------------------------------------------------------------------\n",
    "def Outlier_Check(XYZ,dz,pwr):\n",
    "    dsize    = XYZ.shape\n",
    "    n        = int((dsize[0]))\n",
    "    print(\"All available observations pre-outlier analysis: \" + str(n))\n",
    "    #Pre-alo Variables\n",
    "    IDW      = np.zeros((n,n))\n",
    "    widw     = np.zeros((n,n))\n",
    "    hij      = np.zeros((n,n))\n",
    "    #Loop through each point\n",
    "    for i in range(n):\n",
    "        #Determine euclidean distance (km) from each observation [RadEarth*c/1000]\n",
    "        LonD        = np.radians(XYZ[:,0])\n",
    "        LatD        = np.radians(XYZ[:,1])\n",
    "        delta_phi   = LatD[i]-LatD\n",
    "        delta_lamda = LonD[i]-LonD\n",
    "        a           = np.sin(delta_phi/2)**2 + np.cos(LatD[i])*np.cos(LatD)*np.sin(delta_lamda/2.0)**2\n",
    "        a[a==0]     = np.nan\n",
    "        c           = 2*np.arctan(np.sqrt(a)/np.sqrt(1-a)) #getting errors in a\n",
    "        #Replace these values with nan... avoid error message\n",
    "        c[c==0]     = np.nan\n",
    "        #Determine Inverse Distance Weight between each observation\n",
    "        IDW[i,:]    = 1/(RadEarth*c/1000)**pwr\n",
    "        IDW[i,i]    = np.nan\n",
    "        #Index observations for each stations\n",
    "        widw[:,i]   = XYZ[i,2]\n",
    "        widw[i,i]   = np.nan\n",
    "        #Determine Euclidean distance b/n each station\n",
    "        hij[i,:]    = RadEarth*c/1000\n",
    "        hij[i,i]    = 99999\n",
    "    #Are the observations w/n threshold that has been defined\n",
    "    Eucl_Logical    = hij<dz #getting errors in hij\n",
    "    Sum_row_Eucl    = np.nansum(IDW*Eucl_Logical, axis=1)\n",
    "    #Weighted distance b/n observations as a ratio\n",
    "    Wij = np.full((n, n),np.nan)\n",
    "    for i in range(n):\n",
    "        if Sum_row_Eucl[i] > 0:\n",
    "            Wij[i,:] = np.round(IDW[i,:]/Sum_row_Eucl[i],2)        \n",
    "        Wij[i,i] = np.nan\n",
    "    #Observations with no cooresponding stations within a given threshold will have Wij == inf... \n",
    "    #Replace these values with nan\n",
    "    std_idw           = np.std(XYZ[:,2])\n",
    "    weighted_variable = np.nansum(Wij*widw*Eucl_Logical,axis=1)\n",
    "    #Z-score for a normal distribution\n",
    "    Zscore_i          = (XYZ[:,2]-weighted_variable)/std_idw\n",
    "    non_outliers      = abs(Zscore_i)<1.960\n",
    "    #Return outliers\n",
    "    XYZ_outliers      = XYZ[non_outliers==False]\n",
    "    #Return nonoutliers\n",
    "    XYZ_nonoutliers   = XYZ[non_outliers==True]\n",
    "    print(\"Outliers detected: \" + str(len(XYZ_outliers)))\n",
    "    print(\"Specifications: Interpolation == IDW to the \" + str(pwr) + \" power\")\n",
    "    print(\"Specifications: Search Radius == \" + str(dz) + \"km\")\n",
    "    print(\"----------------------------------------------\")\n",
    "    return(XYZ_nonoutliers,XYZ_outliers)\n",
    "# Adding '0' inch observations function -------------------------------------------------------------------------------\n",
    "def Dumby_Check(lons, lats, query_lon, query_lat, dz):\n",
    "    hij        = np.zeros((len(lons),1))\n",
    "    for z in range(len(lons)):\n",
    "        # Constant\n",
    "        R = RadEarth\n",
    "        # Observation points\n",
    "        lat1 = lats[z]\n",
    "        lon1 = lons[z]\n",
    "        # Query/grid point\n",
    "        lat2 = query_lat\n",
    "        lon2 = query_lon\n",
    "        # Calculation\n",
    "        phi1, phi2 = math.radians(lat1), math.radians(lat2) \n",
    "        dphi       = math.radians(lat2 - lat1)\n",
    "        dlambda    = math.radians(lon2 - lon1)\n",
    "        a          = math.sin(dphi/2)**2 + math.cos(phi1)*math.cos(phi2)*math.sin(dlambda/2)**2\n",
    "        # Euclidean distance in km\n",
    "        hij[z,0]   = ((2*R*math.atan2(math.sqrt(a), math.sqrt(1 - a)))/1000)\n",
    "    hij            = np.sum(hij<=dz)\n",
    "    # Return the number of stations within dz kms of an observation.\n",
    "    return(hij)\n",
    "# Points for interpolation check -------------------------------------------------------------------------------\n",
    "def Density_Check(lons,lats,data,dz,dp):\n",
    "    XYZ      = np.concatenate((np.expand_dims(lons,1), np.expand_dims(lats,1), np.expand_dims(data,1)), axis=1)\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(\"Initial number of observations: \" + str(len(XYZ)))\n",
    "    n        = len(XYZ)\n",
    "    #Pre-alo Variables\n",
    "    hij      = np.zeros((n,n))\n",
    "    #Loop through each point\n",
    "    for i in range(n):\n",
    "        #Determine euclidean distance (km) from each observation [RadEarth*c/1000]\n",
    "        LonD        = np.radians(XYZ[:,0])\n",
    "        LatD        = np.radians(XYZ[:,1])\n",
    "        delta_phi   = LatD[i]-LatD\n",
    "        delta_lamda = LonD[i]-LonD\n",
    "        a           = np.sin(delta_phi/2)**2 + np.cos(LatD[i])*np.cos(LatD)*np.sin(delta_lamda/2.0)**2\n",
    "        a[a==0]     = np.nan\n",
    "        c           = 2*np.arctan(np.sqrt(a)/np.sqrt(1-a)) #getting errors in a\n",
    "        #Replace these values with nan... avoid error message\n",
    "        c[c==0]     = np.nan\n",
    "        #Determine Euclidean distance b/n each station\n",
    "        hij[i,:]    = RadEarth*c/1000\n",
    "        hij[i,i]    = 99999\n",
    "    #Are the observations w/n threshold that has been defined\n",
    "    Eucl_Logical    = hij<dz #getting errors in hij\n",
    "    Eucl_Logical    = np.expand_dims(np.sum(Eucl_Logical,axis=1),1)\n",
    "    XYZ             = np.asarray(list(compress(XYZ,(Eucl_Logical>dp)==True)))\n",
    "    print(\"Clustered number of observations: \" + str(int(len(XYZ))))\n",
    "    print(\"Specifications: Distance == \" + str(dz) + \"km Point Density == \" + str(dp))\n",
    "    print(\"----------------------------------------------\")\n",
    "    return(XYZ)\n",
    "#####################################################################################################################\n",
    "### LOAD AND PRE-PROCESS DATA, AND INTERPOLATE THE DATA\n",
    "#####################################################################################################################\n",
    "Path_Files = 'C:/Users/Mike/Desktop/GHCND/attempt_01/complete_list/'\n",
    "File_List  = find_files(Path_Files, suffix=\".csv\" )\n",
    "for z in range(1):\n",
    "    # Load and pre-process\n",
    "    Testing    = pd.read_csv(Path_Files+File_List[-1])\n",
    "    # Limit area for computation efficiency\n",
    "    Region_Log = np.empty([len(Testing),1], dtype=object)\n",
    "    for x in range(len(Testing)):\n",
    "        if (float(Testing['1'][x]) < North_NE+Buffer_N) and (float(Testing['1'][x]) > South_NE-Buffer_S) and \\\n",
    "           (float(Testing['0'][x]) <  East_NE+Buffer_E) and (float(Testing['0'][x]) >  West_NE-Buffer_W): Region_Log[x,0] = '1'\n",
    "        else: Region_Log[x,0] = '0'\n",
    "    Testing = Testing[Region_Log=='1'].reset_index()\n",
    "    \n",
    "    # Creating some names\n",
    "    File       = File_List[z]\n",
    "    File       = File.replace('complete_list_', '')\n",
    "    File_title = File.replace('_SNOW.csv', '')\n",
    "    File_png   = File.replace('_SNOW.csv', '.png')\n",
    "    File_pdf   = File.replace('_SNOW.csv', '.pdf')\n",
    "    title_str  = File_title[0:4]+\"-\"+File_title[4:6]+\"-\"+File_title[6:8]+\" to \"+File_title[9:13]+\"-\"+File_title[13:15]+\"-\"+File_title[15:17]\n",
    "\n",
    "    # Converting data \n",
    "    lons       = Testing['0'].astype(float)\n",
    "    lats       = Testing['1'].astype(float)\n",
    "    data       = Testing['2'].astype(float)\n",
    "    save_data  = np.concatenate((np.expand_dims(np.asarray(lons),1), np.expand_dims(np.asarray(lats),1), \n",
    "                                         np.expand_dims(np.asarray(data),1)),axis=1)\n",
    "    # Determine outliers\n",
    "    XYZ_nonoutliers,XYZ_outliers  = Outlier_Check(save_data,50,2)\n",
    "    XYZ_outliers_df               = pd.DataFrame(XYZ_outliers)\n",
    "    XYZ_outliers_df.to_csv(\"outliers\"+\"_\"+File, sep=',',index=False)\n",
    "    # Check density\n",
    "    XYZ                                    = Density_Check(XYZ_nonoutliers[:,0],XYZ_nonoutliers[:,1],XYZ_nonoutliers[:,2],50,2) \n",
    "    # Begin to set grid for Kriging interpolation\n",
    "    grid_lon       = np.arange(np.amin(XYZ[:,0]), np.amax(XYZ[:,0]), 0.5) #grid_space is the desired delta/step of the output array \n",
    "    grid_lat       = np.arange(np.amin(XYZ[:,1]), np.amax(XYZ[:,1]), 0.5) #dxdy\n",
    "    xintrp, yintrp = np.meshgrid(grid_lon, grid_lat)\n",
    "    # Add dumby points [obs == 0 inch]\n",
    "    dumby_lon      = []\n",
    "    dumby_lat      = []\n",
    "    n_iterations   = xintrp.shape\n",
    "    for x in range(n_iterations[0]):\n",
    "        for y in range(n_iterations[1]):\n",
    "            query_log = Dumby_Check(XYZ[:,0], XYZ[:,1], xintrp[x,y], yintrp[x,y], 50)\n",
    "            if query_log==0: \n",
    "                dumby_lon.append(xintrp[x,y])\n",
    "                dumby_lat.append(yintrp[x,y])\n",
    "    dumby_data                             = np.concatenate((np.expand_dims(np.asarray(dumby_lon),1), \n",
    "                                                             np.expand_dims(np.asarray(dumby_lat),1), \n",
    "                                                             np.zeros((len(dumby_lat),1))),axis=1)\n",
    "    XYZ_nonoutliers                        = np.concatenate((dumby_data,XYZ_nonoutliers),axis=0)\n",
    "    XYZ_nonoutliers_df                        = pd.DataFrame(XYZ_outliers)\n",
    "    XYZ_nonoutliers_df.to_csv(\"nonoutliers\"+\"_\"+File, sep=',',index=False)\n",
    "    XYZ_nonoutliers = XYZ_nonoutliers[XYZ_nonoutliers[:,2]!=0]\n",
    "    Region_Log      = np.empty([len(XYZ_nonoutliers),1], dtype=object)\n",
    "    for i in range(len(XYZ_nonoutliers)):\n",
    "        if (XYZ_nonoutliers[i,1] < North_NE+Buffer_N) and (XYZ_nonoutliers[i,1] > South_NE-Buffer_S) and \\\n",
    "           (XYZ_nonoutliers[i,0] <  East_NE+Buffer_E) and (XYZ_nonoutliers[i,0] >  West_NE-Buffer_W): Region_Log[i,0] = '1'\n",
    "        else: Region_Log[i,0] = '0'\n",
    "    XYZ_nonoutliers = XYZ_nonoutliers[Region_Log[:,0]=='1']\n",
    "    # Nonoutlier data including 0 dumby points\n",
    "    Lon   = XYZ_nonoutliers[:,0].astype(float)\n",
    "    Lat   = XYZ_nonoutliers[:,1].astype(float)\n",
    "    data  = XYZ_nonoutliers[:,2].astype(float)\n",
    "    # Outlier data \n",
    "    Lon_o   = XYZ_outliers[:,0].astype(float)\n",
    "    Lat_o   = XYZ_outliers[:,1].astype(float)\n",
    "    data_o  = XYZ_outliers[:,2].astype(float)\n",
    "    #####################################################################################################################\n",
    "    ### SPHERICAL KRIGIN INTERPOLATION ONTO GRID\n",
    "    #####################################################################################################################\n",
    "    print('Ord. Kriging #1')\n",
    "    grid_lon   = np.arange(np.amin(Lon), np.amax(Lon), 0.1)#dxkrig\n",
    "    grid_lat   = np.arange(np.amin(Lat), np.amax(Lat), 0.1)\n",
    "    North      = np.amax(Lat)\n",
    "    East       = np.amax(Lon)\n",
    "    South      = np.amin(Lat)\n",
    "    West       = np.amin(Lon)\n",
    "    print('Ord. Kriging #2')\n",
    "    OK      = OrdinaryKriging(Lon, Lat, data, variogram_model='spherical', verbose=True,\n",
    "                                  enable_plotting=False, coordinates_type='geographic')\n",
    "    print('Ord. Kriging #3')\n",
    "    z1, ss1 = OK.execute('grid', grid_lon, grid_lat)\n",
    "    print('Ord. Kriging #4')\n",
    "    xintrp, yintrp = np.meshgrid(grid_lon, grid_lat)\n",
    "    print('Ord. Kriging #5')\n",
    "    z1[z1<0] = 0 #change negative values to 0\n",
    "    #####################################################################################################################\n",
    "    ### PLOTTING\n",
    "    #####################################################################################################################\n",
    "    # Figure\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    # Map Resources\n",
    "    ax  = plt.subplot(1,1,1, projection=ccrs.Mercator())\n",
    "    ax.set_title('Historical Snowfall Interpolation: '+'\\n'+title_str, fontsize=24, fontweight='bold')\n",
    "    ax.set_extent([West_NE, East_NE, South_NE, North_NE])\n",
    "    #ax.add_feature(cfeature.OCEAN, facecolor=\"lightcyan\",  edgecolor=None, zorder=1)\n",
    "    #ax.set_facecolor('xkcd:salmon')\n",
    "    #ax.add_feature(cfeature.LAKES, facecolor=\"lightcyan\",  edgecolor=None, zorder=1)\n",
    "    #ax.add_geometries(Reader(fname_usa).geometries(), ccrs.PlateCarree(), facecolor=\"none\", edgecolor='black', lw=0.7,zorder=3)\n",
    "    #ax.add_geometries(Reader(fname_usa_cnt).geometries(), ccrs.PlateCarree(), facecolor=\"none\", edgecolor='lightgray', lw=0.5,zorder=2)\n",
    "    #ax.add_geometries(Reader(fname_can).geometries(), ccrs.PlateCarree(), facecolor=\"w\", hatch='\\\\\\\\\\\\\\\\',  edgecolor='black', lw=0.7,zorder=1)\n",
    "    ax.add_geometries(Reader(fname_neas_cl).geometries(), ccrs.PlateCarree(), facecolor=\"none\",  edgecolor='black', lw=0.7,zorder=3)\n",
    "    \n",
    "    # Kriging Interpolation Plot\n",
    "    cb = plt.contourf(xintrp, yintrp, z1, np.asarray(clevs), cmap=cmap_snow, norm=norm_snow, vmin = 0, vmax = 96, transform=ccrs.PlateCarree())\n",
    "\n",
    "    # Non-outliers Observations\n",
    "    cb = plt.scatter(Lon[data!=0], Lat[data!=0], c=data[data!=0], s=10, cmap=cmap_snow, norm=norm_snow, alpha=1, edgecolors='k', linewidths=0.5,\n",
    "                         zorder=4,marker='o', transform=ccrs.PlateCarree())\n",
    "    # Outliers Observations\n",
    "    cb = plt.scatter(Lon_o, Lat_o, c=data_o, s=50, cmap=cmap_snow, norm=norm_snow, alpha=1, edgecolors='k', linewidths=0.5,\n",
    "                         zorder=4,marker='X', transform=ccrs.PlateCarree())\n",
    "    # Contours \n",
    "    CS = ax.contour(xintrp, yintrp, z1, [12,24,36,48],linestyles='dashed', colors='k',\n",
    "                        transform=ccrs.PlateCarree())  # Negative contours default to dashed.\n",
    "    ax.clabel(CS, fontsize=16,inline=True, fmt='%1.0f')\n",
    "\n",
    "    # Colorbar\n",
    "    make_colorbar(ax, cb, pad=0)\n",
    "\n",
    "    # Graticule\n",
    "    #gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "    #                      linewidth=0.75, color='gray', alpha=0.8, linestyle='--')\n",
    "    #gl.ylocator = mticker.FixedLocator([22, 28, 32, 36, 40, 44, 48, 52])\n",
    "    #gl.xlocator = mticker.FixedLocator([-106, -100, -94, -88, -82, -76, -70, -64, -58, -52])\n",
    "    #gl.xlabels_top = False\n",
    "    #gl.ylabels_right = False\n",
    "    #gl.xlabel_style = {'size': 15, 'color': 'gray'}\n",
    "    #gl.ylabel_style = {'size': 15, 'color': 'gray'}\n",
    "\n",
    "    # Saving\n",
    "    #fig.savefig(File_png)\n",
    "    #pp = PdfPages(File_pdf)\n",
    "    #pp.savefig(fig)\n",
    "    #pp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krig_hole-effect_20100208_20100211.txt\n",
      "82\n",
      "All available observations pre-outlier analysis: 3991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mike\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:139: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3925\n",
      "Snow Iteration: 1 Total Outliers: 66\n",
      "All available observations pre-outlier analysis: 3925\n",
      "3914\n",
      "Snow Iteration: 2 Total Outliers: 77\n",
      "All available observations pre-outlier analysis: 3914\n",
      "3913\n",
      "Snow Iteration: 3 Total Outliers: 78\n",
      "All available observations pre-outlier analysis: 3913\n",
      "3913\n",
      "Snow Iteration: 4 Total Outliers: 78\n",
      "Number of Observations Post-Outliers: 3923\n",
      "Number of Outliers Post-Outliers: 78\n",
      "SNOWFALL\n",
      "COUNTIES\n",
      "ATLANTIC\n",
      "USA STATES\n",
      "CONTOURS\n",
      "LAKES\n",
      "CANADA\n",
      "OBSERVATIONS\n"
     ]
    }
   ],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from matplotlib.patches import Path, PathPatch\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "from cartopy.io.shapereader import Reader\n",
    "import cartopy.io.shapereader as shpreader\n",
    "from shapely.geometry import MultiLineString\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.ticker as mticker\n",
    "import cartopy.feature as cfeature\n",
    "import pykrige.kriging_tools as kt\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import compress\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib as mpl\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "LIST_DIR = pd.read_csv('C:/Users/Mike/NESIS_Kriging/Final_Interpolations/Listing.csv')\n",
    "TEXT_DIR = 'C:/Users/Mike/NESIS_Kriging/Final_Interpolations/grid_txt/'\n",
    "\n",
    "#####################################################################################################################\n",
    "### SETTING CONSTANTS\n",
    "#####################################################################################################################\n",
    "# Plotting window\n",
    "West_NE    = -82\n",
    "East_NE    = -66\n",
    "South_NE   = 37\n",
    "North_NE   = 48\n",
    "dxdy       = 0.5\n",
    "dxkrig     = 0.1\n",
    "Buffer_E   = 2\n",
    "Buffer_W   = 10\n",
    "Buffer_S   = 10\n",
    "Buffer_N   = 2 \n",
    "\n",
    "# Shapefiles for cartopy\n",
    "shp_path      = 'C:/Users/Mike/Desktop/Graduate_Work/Evaluation_Procedure/NWP_Domains_shp/Boundaries_Shapefiles/'\n",
    "fname_usa     = shp_path+'United_States_States_5m/cb_2016_us_state_5m'\n",
    "fname_can     = shp_path+'Canada_trsmd/canada_tr'\n",
    "fname_usa_cnt = shp_path+'/cb_2018_us_county_5m/cb_2018_us_county_5m.shp'\n",
    "fname_neas_cl = shp_path+'/Northeast_Selection/Northeast_Outline.shp'\n",
    "fname_roads   = shp_path+'/tl_2016_us_primaryroads/tl_2016_us_primaryroads.shp'\n",
    "\n",
    "ocean = cfeature.NaturalEarthFeature(category='physical', name='ocean',\n",
    "                                     scale='50m',\n",
    "                                     facecolor=cfeature.COLORS['water'])\n",
    "\n",
    "Lakes = cfeature.NaturalEarthFeature(category='physical', name='lakes',\n",
    "                                     scale='10m',\n",
    "                                     facecolor='lightcyan')\n",
    "        \n",
    "# Creating colorbar\n",
    "clevs     = [0,0.1,1,2,3,4,6,8,12,18,24,30,36,48,60,72,96]\n",
    "cmap_data = [(0.96456693, 0.96456693, 0.96456693),\n",
    "                 (0.77952756, 0.86614173, 0.92125984),\n",
    "                 (0.51574803, 0.73228346, 0.86220472),\n",
    "                 (0.32283465, 0.58661417, 0.77952756),\n",
    "                 (0.18897638, 0.42913386, 0.66535433),\n",
    "                 (0.17716535, 0.28740157, 0.63385827),\n",
    "                 (1.        , 1.        , 0.6496063 ),\n",
    "                 (1.        , 0.80314961, 0.16535433),\n",
    "                 (1.        , 0.6023622 , 0.16141732),\n",
    "                 (0.88188976, 0.21259843, 0.15748031),\n",
    "                 (0.68503937, 0.15354331, 0.16929134),\n",
    "                 (0.50787402, 0.16535433, 0.16535433),\n",
    "                 (0.33858268, 0.16535433, 0.16535433),\n",
    "                 (0.83070866, 0.83070866, 1.        ),\n",
    "                 (0.68110236, 0.62204724, 0.87007874),\n",
    "                 (0.57086614, 0.43307087, 0.7007874 )]\n",
    "cmap_snow      = mcolors.ListedColormap(cmap_data, 'precipitation')\n",
    "norm_snow      = mcolors.BoundaryNorm(clevs, cmap_snow.N)\n",
    "#####################################################################################################################\n",
    "### DEFINING FUNCTIONS\n",
    "#####################################################################################################################\n",
    "# Colorbar function -------------------------------------------------------------------------------\n",
    "def make_colorbar(ax, mappable, **kwargs):\n",
    "    divider = make_axes_locatable(ax)\n",
    "    orientation = kwargs.pop('orientation', 'vertical')\n",
    "    if orientation == 'vertical':\n",
    "        loc = 'right'\n",
    "    elif orientation == 'horizontal':\n",
    "        loc = 'bottom'\n",
    "    cax = divider.append_axes(loc, '5%', pad='3%', axes_class=mpl.pyplot.Axes)\n",
    "    ax.get_figure().colorbar(mappable, cax=cax, orientation=orientation,label=\"inch\",\n",
    "                                 ticks=[0,0.1,1,2,3,4,6,8,12,18,24,30,36,48,60,72,96])\n",
    "\n",
    "from datetime import datetime\n",
    "def Title_STR(dt_start,dt_end):\n",
    "    START  = dt_start.split(\"/\")\n",
    "    START  = datetime(int(START[2]),int(START[0]),int(START[1]))\n",
    "    START  = START.strftime(\"%B %d, %Y,\")\n",
    "    END    = dt_end.split(\"/\")\n",
    "    END    = datetime(int(END[2]),int(END[0]),int(END[1]))\n",
    "    END    = END.strftime(\"%B %d, %Y\")\n",
    "    STRING = START+\" to \"+END\n",
    "    return(STRING)\n",
    "\n",
    "RadEarth   = 6371000 #m Euclidean Dist.\n",
    "def Outlier_Check(XYZ,dz,pwr):\n",
    "    dsize    = XYZ.shape\n",
    "    n        = int((dsize[0]))\n",
    "    print(\"All available observations pre-outlier analysis: \" + str(n))\n",
    "    #Pre-alo Variables\n",
    "    IDW      = np.zeros((n,n))\n",
    "    widw     = np.zeros((n,n))\n",
    "    hij      = np.zeros((n,n))\n",
    "    #Loop through each point\n",
    "    for i in range(n):\n",
    "        #Determine euclidean distance (km) from each observation [RadEarth*c/1000]\n",
    "        LonD        = np.radians(XYZ[:,0])\n",
    "        LatD        = np.radians(XYZ[:,1])\n",
    "        delta_phi   = LatD[i]-LatD\n",
    "        delta_lamda = LonD[i]-LonD\n",
    "        a           = np.sin(delta_phi/2)**2 + np.cos(LatD[i])*np.cos(LatD)*np.sin(delta_lamda/2.0)**2\n",
    "        a[a==0]     = np.nan\n",
    "        c           = 2*np.arctan(np.sqrt(a)/np.sqrt(1-a)) #getting errors in a\n",
    "        #Replace these values with nan... avoid error message\n",
    "        c[c==0]     = np.nan\n",
    "        #Determine Inverse Distance Weight between each observation\n",
    "        IDW[i,:]    = 1/(RadEarth*c/1000)**pwr\n",
    "        IDW[i,i]    = np.nan\n",
    "        #Index observations for each stations\n",
    "        widw[:,i]   = XYZ[i,2]\n",
    "        widw[i,i]   = np.nan\n",
    "        #Determine Euclidean distance b/n each station\n",
    "        hij[i,:]    = RadEarth*c/1000\n",
    "        hij[i,i]    = 99999\n",
    "    #Are the observations w/n threshold that has been defined\n",
    "    Eucl_Logical    = hij<dz #getting errors in hij\n",
    "    Sum_row_Eucl    = np.nansum(IDW*Eucl_Logical, axis=1)\n",
    "    #Weighted distance b/n observations as a ratio\n",
    "    Wij = np.full((n, n),np.nan)\n",
    "    for i in range(n):\n",
    "        if Sum_row_Eucl[i] > 0:\n",
    "            Wij[i,:] = np.round(IDW[i,:]/Sum_row_Eucl[i],2)\n",
    "        Wij[i,i] = np.nan\n",
    "    #Observations with no cooresponding stations within a given threshold will have Wij == inf...\n",
    "    #Replace these values with nan\n",
    "    std_idw           = np.std(XYZ[:,2])\n",
    "    weighted_variable = np.nansum(Wij*widw*Eucl_Logical,axis=1)\n",
    "    #Z-score for a normal distribution\n",
    "    Zscore_i          = (XYZ[:,2]-weighted_variable)/std_idw\n",
    "    Logical_array     = [np.logical_and(Zscore_i > -1.280, Zscore_i < 4)]\n",
    "    #non_outliers      = abs(Zscore_i)<1.960\n",
    "    #Return outliers\n",
    "    XYZ_outliers      = XYZ[Logical_array[0]==False]\n",
    "    #Return nonoutliers\n",
    "    XYZ_nonoutliers   = XYZ[Logical_array[0]==True]\n",
    "    #print(\"Outliers detected: \" + str(len(XYZ_outliers)))\n",
    "    #print(\"Specifications: Interpolation == IDW to the \" + str(pwr) + \" power\")\n",
    "    #print(\"Specifications: Search Radius == \" + str(dz) + \"km\")\n",
    "    #print(\"----------------------------------------------\")\n",
    "    return(XYZ_nonoutliers,XYZ_outliers,Zscore_i)\n",
    "\n",
    "Kriging_Grid = 0.025\n",
    "West_NE      = -82\n",
    "East_NE      = -66\n",
    "South_NE     = 37\n",
    "North_NE     = 48\n",
    "#####################################################################################################################\n",
    "### PLOTTING\n",
    "#####################################################################################################################\n",
    "Kriging_Grid = 0.025\n",
    "grid_lon     = np.arange(West_NE , East_NE , Kriging_Grid)\n",
    "grid_lat     = np.arange(South_NE, North_NE, Kriging_Grid)\n",
    "xintrp, yintrp = np.meshgrid(grid_lon, grid_lat)\n",
    "\n",
    "for i in range(82,83): #80\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    print(LIST_DIR['Grid File'][i])\n",
    "    print(i)\n",
    "    now       = datetime.now()\n",
    "    GRID      = 'C:/Users/Mike/NESIS_Kriging/Final_Interpolations/grid_txt/'+LIST_DIR['Folder'][i]+'/'+LIST_DIR['Grid File'][i]\n",
    "    ascii_grid= np.loadtxt(GRID, skiprows=0)\n",
    "    OBS  = 'C:/Users/Mike/NESIS_Kriging/Final_Interpolations/grid_obs/'+LIST_DIR['OBS File'][i]\n",
    "    XYZ  = pd.read_csv(OBS,skiprows=[0],names=['Lon','Lat','Snowfall_inch'])\n",
    "    XYZ  = np.asarray(XYZ).astype(float)\n",
    "    #OUTLIERS\n",
    "    XYZ_obs        = []\n",
    "    XYZ_out        = []\n",
    "    do             = len(XYZ)\n",
    "    doutlier       = [np.nan,np.nan] #start\n",
    "    i              = 0\n",
    "    while (doutlier[-1]-doutlier[-2])!= 0:\n",
    "        i                         = i+1\n",
    "        XYZ,XYZ_outliers,Zscore_i = Outlier_Check(XYZ,50,2)\n",
    "        print(len(XYZ))\n",
    "        doutlier.append(do-len(XYZ))\n",
    "        XYZ_obs.append(XYZ)\n",
    "        XYZ_out.append(XYZ_outliers)\n",
    "        print(\"Snow Iteration: \"+str(i)+\" Total Outliers: \"+str(doutlier[-1]))\n",
    "        if i>10: break\n",
    "    XYZ_obs = np.unique(np.vstack(XYZ_obs),axis=0)\n",
    "    XYZ_out = np.unique(np.vstack(XYZ_out),axis=0)\n",
    "    print(\"Number of Observations Post-Outliers: \"+str(len(XYZ_obs)))\n",
    "    print(\"Number of Outliers Post-Outliers: \"+str(len(XYZ_out)))\n",
    "    #naming\n",
    "    test = LIST_DIR['OBS File'][i]\n",
    "    test = test.replace(\"complete_list_\",\"map_\")\n",
    "    test = test.replace(\"_SNOW.csv\",\"\")\n",
    "    # Map Resources\n",
    "    ax  = plt.subplot(1,1,1, projection=ccrs.Mercator())\n",
    "    ax.set_extent([West_NE, East_NE, South_NE, North_NE],crs=ccrs.PlateCarree())\n",
    "    ax.set_title(Title_STR(LIST_DIR['START'][i],LIST_DIR['END'][i]),\n",
    "                 fontsize=24,loc='left',fontweight='bold',color=\"midnightblue\")\n",
    "    #SNOWFALL - SHADED (1)\n",
    "    cb = plt.contourf(xintrp, yintrp, ascii_grid, np.asarray(clevs), cmap=cmap_snow, norm=norm_snow,zorder=1,\n",
    "                      vmin = 0, vmax = 96, alpha=1,extend=max, transform=ccrs.PlateCarree())\n",
    "    print(\"SNOWFALL\")\n",
    "    #USA COUNTIES - POLYGON (2)\n",
    "    ax.add_geometries(Reader(fname_usa_cnt).geometries(), ccrs.PlateCarree(), facecolor=\"none\",zorder=2,\n",
    "                      edgecolor='silver', lw=0.25, alpha=0.80)\n",
    "    print(\"COUNTIES\")\n",
    "    #ATLANTIC OCEAN - POLYGON (3)\n",
    "    ax.add_feature(ocean,zorder=5)    #3\n",
    "    print(\"ATLANTIC\")\n",
    "    #USA STATES - POLYGON (4)\n",
    "    ax.add_geometries(Reader(fname_usa).geometries(), ccrs.PlateCarree(),zorder=7, #4\n",
    "                      facecolor=\"none\", edgecolor='k', lw=1)\n",
    "    print(\"USA STATES\")\n",
    "    #SNOW CONTOURS - SEGMENTS (5)\n",
    "    CS = ax.contour(xintrp, yintrp, ascii_grid,  [12,24,36,48], linestyles='dashed', colors='k', zorder=3, \n",
    "                        transform=ccrs.PlateCarree())\n",
    "    ax.clabel(CS, fontsize=16, inline=True, fmt='%1.0f') \n",
    "    print(\"CONTOURS\")\n",
    "    #LAKES - POLYGON (6)\n",
    "    ax.add_feature(Lakes,zorder=4) #5\n",
    "    print(\"LAKES\")\n",
    "    #CANADA PROVINCES - POLYGON (7)\n",
    "    ax.add_geometries(Reader(fname_can).geometries(), ccrs.PlateCarree(), facecolor=\"w\", hatch='\\\\\\\\\\\\\\\\',\n",
    "                      edgecolor='black', lw=0.7,zorder=6)\n",
    "    print(\"CANADA\")\n",
    "    #OBS - POINT (8)\n",
    "    #XYZ_nonoutliers,XYZ_outliers,Zscore_i = Outlier_Check(XYZ,50,2)\n",
    "    cb = plt.scatter(XYZ_obs[:,0], XYZ_obs[:,1], c=XYZ_obs[:,2], s=10, \n",
    "                     cmap=cmap_snow, norm=norm_snow, zorder=8, \n",
    "                     edgecolors='k', linewidths=0.25, marker='o', transform=ccrs.PlateCarree())\n",
    "    #OUTLIERS - POINT (9)\n",
    "    cb = plt.scatter(XYZ_out[:,0], XYZ_out[:,1], c=XYZ_out[:,2], s=25, \n",
    "                     cmap=cmap_snow, norm=norm_snow, zorder=9,\n",
    "                     edgecolors='k', linewidths=0.25,marker='X', transform=ccrs.PlateCarree())\n",
    "    print(\"OBSERVATIONS\")\n",
    "    #USA ROADS - SEGMENTS (10)\n",
    "    #shp   = shpreader.Reader(fname_roads)\n",
    "    #add_s = shp.records()\n",
    "    #for add in add_s:\n",
    "    #    if( add.geometry == None):\n",
    "    #        pass\n",
    "    #    else:\n",
    "    #        lines = add.geometry\n",
    "    #        line_good=[]\n",
    "    #        for l in lines:\n",
    "    #            start_pt = list(l.coords)[0]\n",
    "    #            for i in range(1,len(l.coords)):\n",
    "    #                end_pt = list(l.coords)[i]\n",
    "    #                simple_line = (start_pt, end_pt)\n",
    "    #                line_good.append(simple_line)\n",
    "    #                start_pt = end_pt\n",
    "    #        lines = MultiLineString(line_good)\n",
    "    #        ax.add_geometries([lines], ccrs.PlateCarree(), edgecolor='slategray', facecolor=None, lw=0.75, zorder=10)   \n",
    "    #print(\"ROADS\")\n",
    "    #COLORBAR\n",
    "    make_colorbar(ax, cb, pad=0)\n",
    "    #ADJUST FIGURE\n",
    "    plt.subplots_adjust(top = 0.95, bottom = 0.05, right = 1, left = 0, hspace = 0, wspace = 0)\n",
    "    #ADD FIG TEXT\n",
    "    fig.text(0.09,0.02,'Kriging Resolution: 0.025$^\\circ$x0.025$^\\circ$', ha='left')\n",
    "    dt_string = now.strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "    fig.text(0.85,0.02,\"Created: \"+dt_string, ha='right')\n",
    "    #SAVE FIGURE\n",
    "    fig.savefig(test,dpi=300)\n",
    "    #CLOSE FGURE\n",
    "    plt.close(fig=None)\n",
    "    #time.sleep(60*4) # second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kriging_Grid = 0.025\n",
    "West_NE      = -82\n",
    "East_NE      = -66\n",
    "South_NE     = 37\n",
    "North_NE     = 48\n",
    "\n",
    "grid_lon       = np.arange(West_NE , East_NE , Kriging_Grid)\n",
    "grid_lat       = np.arange(South_NE, North_NE, Kriging_Grid)\n",
    "xintrp, yintrp = np.meshgrid(grid_lon, grid_lat)\n",
    "density        = np.zeros(xintrp.shape)\n",
    "\n",
    "Dir = 'C:/Users/Mike/NESIS_Kriging/complete_list/'\n",
    "LS  = find_files(Dir, suffix=\".csv\")\n",
    "for i in range(len(LS)):\n",
    "    print(i)\n",
    "    LS_i  = pd.read_csv(Dir+LS[i])\n",
    "    Lon_i = LS_i['0']\n",
    "    Lat_i = LS_i['1']\n",
    "    for j in range(len(LS_i)):\n",
    "        q_lon_j = abs(Lon_i[j]-xintrp)\n",
    "        q_lat_j = abs(Lat_i[j]-yintrp)\n",
    "        q_coord = q_lon_j+q_lat_j\n",
    "        idx_xy  = np.unravel_index(q_coord.argmin(),q_coord.shape)\n",
    "        #print(idx_xy)\n",
    "        density[idx_xy[0],idx_xy[1]] = density[idx_xy[0],idx_xy[1]] +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-29_2020-10-30\n",
      "2019-11-29_2019-12-03\n",
      "2019-01-18_2019-01-21\n",
      "2018-11-14_2018-11-16\n",
      "2018-03-20_2018-03-22\n",
      "2018-03-11_2018-03-15\n",
      "2018-03-05_2018-03-08\n",
      "2018-03-01_2018-03-03\n",
      "2018-01-03_2018-01-05\n",
      "2017-03-12_2017-03-15\n",
      "2017-02-09_2017-02-10\n",
      "2016-11-17_2016-11-22\n",
      "2016-01-22_2016-01-24\n",
      "2015-02-14_2015-02-16\n",
      "2015-02-08_2015-02-10\n",
      "2015-01-29_2015-02-03\n",
      "2015-01-25_2015-01-28\n",
      "2014-12-09_2014-12-14\n",
      "2014-11-26_2014-11-28\n",
      "2014-02-11_2014-02-14\n",
      "2014-01-20_2014-01-22\n",
      "2013-12-30_2014-01-03\n",
      "2013-12-13_2013-12-16\n",
      "2013-03-17_2013-03-20\n",
      "2013-03-03_2013-03-09\n",
      "2013-02-08_2013-02-10\n",
      "2012-12-28_2012-12-31\n",
      "2012-12-24_2012-12-28\n",
      "2011-10-25_2011-10-31\n",
      "2011-02-24_2011-02-27\n",
      "2011-02-01_2011-02-04\n",
      "2011-01-26_2011-01-27\n",
      "2011-01-09_2011-01-13\n",
      "2010-12-24_2010-12-28\n",
      "2010-02-21_2010-03-01\n",
      "2010-02-12_2010-02-19\n",
      "2010-02-08_2010-02-11\n",
      "2010-02-04_2010-02-08\n",
      "2009-12-28_2010-01-04\n",
      "2009-12-18_2009-12-21\n",
      "2009-12-07_2009-12-11\n",
      "2009-02-26_2009-03-03\n",
      "2009-02-22_2009-02-24\n",
      "2008-12-21_2008-12-23\n",
      "2008-12-18_2008-12-22\n",
      "2007-12-14_2007-12-17\n",
      "2007-11-30_2007-12-04\n",
      "2007-04-03_2007-04-06\n",
      "2007-03-16_2007-03-18\n",
      "2007-02-11_2007-02-16\n",
      "2006-02-10_2006-02-14\n",
      "2005-02-28_2005-03-02\n",
      "2005-01-22_2005-01-24\n",
      "2003-12-04_2003-12-08\n",
      "2003-02-14_2003-02-18\n",
      "2003-01-01_2003-01-04\n",
      "2002-12-23_2002-12-26\n",
      "2000-12-28_2001-01-01\n",
      "2000-02-16_2000-02-20\n",
      "2000-01-24_2000-02-01\n",
      "2000-01-24_2000-01-27\n",
      "1999-03-12_1999-03-16\n",
      "1999-01-13_1999-01-16\n",
      "1997-03-31_1997-04-01\n",
      "1997-01-08_1997-01-12\n",
      "1996-04-09_1996-04-11\n",
      "1996-03-03_1996-03-09\n",
      "1996-02-01_1996-02-04\n",
      "1996-01-06_1996-01-09\n",
      "1995-12-18_1995-12-22\n",
      "1995-02-02_1995-02-06\n",
      "1994-02-28_1994-03-04\n",
      "1994-02-22_1994-02-25\n",
      "1994-01-16_1994-01-18\n",
      "1994-01-04_1994-01-09\n",
      "1994-01-01_1994-01-05\n",
      "1993-03-12_1993-03-15\n",
      "1993-02-20_1993-02-24\n",
      "1993-02-14_1993-02-18\n",
      "1992-12-09_1992-12-13\n",
      "1990-12-27_1990-12-29\n",
      "1988-02-09_1988-02-14\n",
      "1988-01-22_1988-01-27\n",
      "1988-01-05_1988-01-09\n",
      "1987-12-13_1987-12-17\n",
      "1987-11-10_1987-11-12\n",
      "1987-02-22_1987-02-24\n",
      "1987-01-21_1987-01-24\n",
      "1987-01-08_1987-01-12\n",
      "1987-01-01_1987-01-03\n",
      "1985-03-01_1985-03-05\n",
      "1985-01-29_1985-02-03\n",
      "1984-02-24_1984-03-01\n",
      "1983-02-10_1983-02-13\n",
      "1982-04-04_1982-04-08\n",
      "1982-01-20_1982-01-24\n",
      "1982-01-12_1982-01-15\n",
      "1979-02-17_1979-02-20\n",
      "1979-02-06_1979-02-09\n",
      "1978-02-04_1978-02-08\n",
      "1978-01-17_1978-01-21\n",
      "1978-01-14_1978-01-19\n",
      "1978-01-11_1978-01-15\n",
      "1977-03-21_1977-03-25\n",
      "1977-01-07_1977-01-11\n",
      "1974-01-08_1974-01-12\n",
      "1973-12-15_1973-12-18\n",
      "1972-02-16_1972-02-20\n",
      "1971-11-23_1971-11-27\n",
      "1971-02-26_1971-03-06\n",
      "1970-12-31_1971-01-02\n",
      "1969-12-25_1969-12-29\n",
      "1969-02-25_1969-03-04\n",
      "1969-02-22_1969-02-28\n",
      "1969-02-08_1969-02-10\n",
      "1967-03-20_1967-03-23\n",
      "1967-02-06_1967-02-08\n",
      "1966-12-22_1966-12-26\n",
      "1966-02-23_1966-02-26\n",
      "1966-01-28_1966-02-01\n",
      "1966-01-21_1966-01-24\n",
      "1964-02-18_1964-02-21\n",
      "1964-01-09_1964-01-14\n",
      "1963-12-21_1963-12-24\n",
      "1962-03-05_1962-03-09\n",
      "1962-02-28_1962-03-07\n",
      "1962-02-13_1962-02-16\n",
      "1961-02-01_1961-02-05\n",
      "1961-01-18_1961-01-21\n",
      "1960-12-10_1960-12-13\n",
      "1960-02-29_1960-03-05\n",
      "1960-02-12_1960-02-15\n",
      "1959-03-12_1959-03-14\n",
      "1958-03-18_1958-03-23\n",
      "1958-02-12_1958-02-18\n",
      "1957-12-03_1957-12-05\n",
      "1956-03-18_1956-03-20\n",
      "1956-03-14_1956-03-17\n",
      "1956-03-03_1956-03-09\n",
      "1952-02-17_1952-02-18\n",
      "1951-12-13_1951-12-16\n",
      "1950-11-22_1950-11-30\n",
      "1950-02-11_1950-02-17\n",
      "1949-01-29_1949-02-01\n",
      "1948-01-23_1948-01-25\n",
      "1947-12-25_1947-12-27\n",
      "1947-02-27_1947-03-04\n",
      "1947-02-20_1947-02-24\n",
      "1946-02-15_1946-02-21\n",
      "1945-12-17_1945-12-20\n",
      "1945-01-13_1945-01-17\n",
      "1944-12-08_1944-12-13\n",
      "1944-02-08_1944-02-13\n",
      "1943-01-25_1943-01-29\n",
      "1942-03-28_1942-03-31\n",
      "1942-02-28_1942-03-04\n",
      "1941-03-07_1941-03-10\n",
      "1940-02-13_1940-02-15\n",
      "1938-11-23_1938-11-25\n",
      "1936-02-10_1936-02-15\n",
      "1936-01-18_1936-01-21\n",
      "1935-01-21_1935-01-25\n",
      "1934-02-23_1934-02-27\n",
      "1933-12-26_1933-12-27\n",
      "1932-12-16_1932-12-18\n",
      "1931-03-04_1931-03-12\n",
      "1929-12-19_1929-12-24\n",
      "1929-02-20_1929-02-22\n",
      "1927-02-16_1927-02-21\n",
      "1926-02-03_1926-02-05\n",
      "1926-01-07_1926-01-10\n",
      "1925-01-28_1925-01-31\n",
      "1924-12-31_1925-01-03\n",
      "1924-04-01_1924-04-04\n",
      "1924-02-17_1924-02-21\n",
      "1923-02-03_1923-02-07\n",
      "1922-01-26_1922-01-30\n",
      "1921-02-18_1921-02-22\n",
      "1920-02-04_1920-02-07\n",
      "1918-01-25_1918-01-29\n",
      "1918-01-21_1918-01-23\n",
      "1918-01-12_1918-01-16\n",
      "1917-12-12_1917-12-15\n",
      "1917-12-06_1917-12-09\n",
      "1917-03-02_1917-03-06\n",
      "1916-03-02_1916-03-09\n",
      "1915-12-10_1915-12-15\n",
      "1915-04-03_1915-04-05\n",
      "1915-03-02_1915-03-08\n",
      "1915-01-29_1915-02-03\n",
      "1914-02-12_1914-02-15\n",
      "1910-02-16_1910-02-18\n",
      "1910-02-10_1910-02-13\n",
      "1910-01-12_1910-01-15\n",
      "1909-12-23_1909-12-27\n",
      "1909-03-02_1909-03-05\n",
      "1909-01-27_1909-01-31\n",
      "1909-01-10_1909-01-15\n",
      "1908-02-16_1908-02-20\n",
      "1908-02-03_1908-02-07\n",
      "1908-01-29_1908-02-02\n",
      "1907-02-04_1907-02-06\n",
      "1906-03-17_1906-03-20\n",
      "1906-03-12_1906-03-17\n",
      "1904-01-27_1904-01-30\n",
      "1903-02-14_1903-02-18\n",
      "1902-12-11_1902-12-14\n",
      "1902-03-03_1902-03-06\n",
      "1902-02-13_1902-02-19\n",
      "1901-02-01_1901-02-06\n",
      "1900-03-15_1900-03-16\n",
      "1900-02-26_1900-03-03\n"
     ]
    }
   ],
   "source": [
    "event_start = [[10,29,2020],[11,29,2019],[1,18,2019],[11,14,2018],[3,20,2018],[3,11,2018],[3,5,2018],[3,1,2018],[1,3,2018],\n",
    "                [3,12,2017],[2,9,2017],[11,17,2016],[1,22,2016],[2,14,2015],[2,8,2015],[1,29,2015],[1,25,2015],\n",
    "                [12,9,2014],[11,26,2014],[2,11,2014],[1,20,2014],[12,30,2013],[12,13,2013],[3,17,2013],[3,3,2013],\n",
    "                [2,8,2013],[12,28,2012],[12,24,2012],[10,25,2011],[2,24,2011],[2,1,2011],[1,26,2011],[1,9,2011],\n",
    "                [12,24,2010],[2,21,2010],[2,12,2010],[2,8,2010],[2,4,2010],[12,28,2009],[12,18,2009],[12,7,2009],\n",
    "                [2,26,2009],[2,22,2009],[12,21,2008],[12,18,2008],[12,14,2007],[11,30,2007],[4,3,2007],[3,16,2007],\n",
    "                [2,11,2007],[2,10,2006],[2,28,2005],[1,22,2005],[12,4,2003],[2,14,2003],[1,1,2003],[12,23,2002],[12,28,2000],[2,16,2000],[1,24,2000],[1,24,2000],\n",
    "                [3,12,1999],[1,13,1999],[3,31,1997],[1,8,1997],[4,9,1996],[3,3,1996],[2,1,1996],[1,6,1996],[12,18,1995],\n",
    "                [2,2,1995],[2,28,1994],[2,22,1994],[1,16,1994],[1,4,1994],[1,1,1994],[3,12,1993],[2,20,1993],[2,14,1993],\n",
    "                [12,9,1992],[12,27,1990],[2,9,1988],[1,22,1988],[1,5,1988],[12,13,1987],[11,10,1987],[2,22,1987],[1,21,1987],\n",
    "                [1,8,1987],[1,1,1987],[3,1,1985],[1,29,1985],[2,24,1984],[2,10,1983],[4,4,1982],[1,20,1982],[1,12,1982],\n",
    "                [2,17,1979],[2,6,1979],[2,4,1978],[1,17,1978],[1,14,1978],[1,11,1978],[3,21,1977],[1,7,1977],[1,8,1974],\n",
    "                [12,15,1973],[2,16,1972],[11,23,1971],[2,26,1971],[12,31,1970],[12,25,1969],[2,25,1969],[2,22,1969],[2,8,1969],\n",
    "                [3,20,1967],[2,6,1967],[12,22,1966],[2,23,1966],[1,28,1966],[1,21,1966],[2,18,1964],[1,9,1964],[12,21,1963],\n",
    "                [3,5,1962],[2,28,1962],[2,13,1962],[2,1,1961],[1,18,1961],[12,10,1960],[2,29,1960],[2,12,1960],[3,12,1959],\n",
    "                [3,18,1958],[2,12,1958],[12,3,1957],[3,18,1956],[3,14,1956],[3,3,1956],[2,17,1952],[12,13,1951],[11,22,1950],\n",
    "                [2,11,1950],[1,29,1949],[1,23,1948],[12,25,1947],[2,27,1947],[2,20,1947],[2,15,1946],[12,17,1945],[1,13,1945],\n",
    "                [12,8,1944],[2,8,1944],[1,25,1943],[3,28,1942],[2,28,1942],[3,7,1941],[2,13,1940],[11,23,1938],[2,10,1936],\n",
    "                [1,18,1936],[1,21,1935],[2,23,1934],[12,26,1933],[12,16,1932],[3,4,1931],[12,19,1929],[2,20,1929],[2,16,1927],\n",
    "                [2,3,1926],[1,7,1926],[1,28,1925],[12,31,1924],[4,1,1924],[2,17,1924],[2,3,1923],[1,26,1922],[2,18,1921],[2,4,1920],\n",
    "                [1,25,1918],[1,21,1918],[1,12,1918],[12,12,1917],[12,6,1917],[3,2,1917],[3,2,1916],[12,10,1915],[4,3,1915],[3,2,1915],\n",
    "                [1,29,1915],[2,12,1914],[2,16,1910],[2,10,1910],[1,12,1910],[12,23,1909],[3,2,1909],[1,27,1909],[1,10,1909],[2,16,1908],\n",
    "                [2,3,1908],[1,29,1908],[2,4,1907],[3,17,1906],[3,12,1906],[1,27,1904],[2,14,1903],[12,11,1902],[3,3,1902],[2,13,1902],\n",
    "                [2,1,1901],[3,15,1900],[2,26,1900]]\n",
    "event_end   = [[10,30,2020],[12,3,2019],[1,21,2019],[11,16,2018],[3,22,2018],[3,15,2018],[3,8,2018],[3,3,2018],[1,5,2018],\n",
    "                [3,15,2017],[2,10,2017],[11,22,2016],[1,24,2016],[2,16,2015],[2,10,2015],[2,3,2015],[1,28,2015],\n",
    "                [12,14,2014],[11,28,2014],[2,14,2014],[1,22,2014],[1,3,2014],[12,16,2013],[3,20,2013],[3,9,2013],\n",
    "                [2,10,2013],[12,31,2012],[12,28,2012],[10,31,2011],[2,27,2011],[2,4,2011],[1,27,2011],[1,13,2011],\n",
    "                [12,28,2010],[3,1,2010],[2,19,2010],[2,11,2010],[2,8,2010],[1,4,2010],[12,21,2009],[12,11,2009],\n",
    "                [3,3,2009],[2,24,2009],[12,23,2008],[12,22,2008],[12,17,2007],[12,4,2007],[4,6,2007],[3,18,2007],\n",
    "                [2,16,2007],[2,14,2006],[3,2,2005],[1,24,2005], [12,8,2003],[2,18,2003],[1,4,2003],[12,26,2002],[1,1,2001],[2,20,2000],[2,1,2000],[1,27,2000],\n",
    "                [3,16,1999],[1,16,1999],[4,1,1997],[1,12,1997],[4,11,1996],[3,9,1996],[2,4,1996],[1,9,1996],\n",
    "                [12,22,1995],[2,6,1995],[3,4,1994],[2,25,1994],[1,18,1994],[1,9,1994],[1,5,1994],[3,15,1993],\n",
    "                [2,24,1993],[2,18,1993],[12,13,1992],[12,29,1990],[2,14,1988],[1,27,1988],[1,9,1988],[12,17,1987],\n",
    "                [11,12,1987],[2,24,1987],[1,24,1987],[1,12,1987],[1,3,1987],[3,5,1985],[2,3,1985],[3,1,1984],\n",
    "                [2,13,1983],[4,8,1982],[1,24,1982],[1,15,1982],[2,20,1979],[2,9,1979],[2,8,1978],[1,21,1978],\n",
    "                [1,19,1978],[1,15,1978],[3,25,1977],[1,11,1977],[1,12,1974],[12,18,1973],[2,20,1972],[11,27,1971],\n",
    "                [3,6,1971],[1,2,1971],[12,29,1969],[3,4,1969],[2,28,1969],[2,10,1969],[3,23,1967],[2,8,1967],\n",
    "                [12,26,1966],[2,26,1966],[2,1,1966],[1,24,1966],[2,21,1964],[1,14,1964],[12,24,1963],[3,9,1962],\n",
    "                [3,7,1962],[2,16,1962],[2,5,1961],[1,21,1961],[12,13,1960],[3,5,1960],[2,15,1960],[3,14,1959],\n",
    "                [3,23,1958],[2,18,1958],[12,5,1957],[3,20,1956],[3,17,1956],[3,9,1956],[2,18,1952],[12,16,1951],\n",
    "                [11,30,1950],[2,17,1950],[2,1,1949],[1,25,1948],[12,27,1947],[3,4,1947],[2,24,1947],[2,21,1946],\n",
    "                [12,20,1945],[1,17,1945],[12,13,1944],[2,13,1944],[1,29,1943],[3,31,1942],[3,4,1942],[3,10,1941],\n",
    "                [2,15,1940],[11,25,1938],[2,15,1936],[1,21,1936],[1,25,1935],[2,27,1934],[12,27,1933],[12,18,1932],\n",
    "                [3,12,1931],[12,24,1929],[2,22,1929],[2,21,1927],[2,5,1926],[1,10,1926],[1,31,1925],[1,3,1925],\n",
    "                [4,4,1924],[2,21,1924],[2,7,1923],[1,30,1922],[2,22,1921],[2,7,1920],[1,29,1918],[1,23,1918],\n",
    "                [1,16,1918],[12,15,1917],[12,9,1917],[3,6,1917],[3,9,1916],[12,15,1915],[4,5,1915],[3,8,1915],\n",
    "                [2,3,1915],[2,15,1914],[2,18,1910],[2,13,1910],[1,15,1910],[12,27,1909],[3,5,1909],[1,31,1909],\n",
    "                [1,15,1909],[2,20,1908],[2,7,1908],[2,2,1908],[2,6,1907],[3,20,1906],[3,17,1906],[1,30,1904],\n",
    "                [2,18,1903],[12,14,1902],[3,6,1902],[2,19,1902],[2,6,1901],[3,16,1900],[3,3,1900]]\n",
    "for i in range(len(event_end)):\n",
    "    # Window of analysis\n",
    "    D_Start   = datetime(event_start[i][2],event_start[i][0],event_start[i][1]).date()\n",
    "    D_Start   = YYYYMMDD_string(D_Start) \n",
    "    D_End     = datetime(event_end[i][2],event_end[i][0],event_end[i][1]).date()\n",
    "    D_End     = YYYYMMDD_string(D_End)\n",
    "    #CSV_PRINT = YYYYMMDD_string(D_Start) + \"_\" + YYYYMMDD_string(D_End)\n",
    "    print(D_Start[0:4]+\"-\"+D_Start[4:6]+\"-\"+D_Start[6:8]+\"_\"+D_End[0:4]+\"-\"+D_End[4:6]+\"-\"+D_End[6:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 Missing Stations!!!\n",
      "Bangor Fss MAINE Penobscot County\n",
      "Franklin MAINE Washington County\n",
      "Copy and paste the above stations to: https://geocode.localfocus.nl/\n",
      "Add to stations list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mike\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:89: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Mike\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import math\n",
    "import re\n",
    "\n",
    "###GIVEN ONE POINT, A DIRECTION, AND DISTANCE, WHAT IS THE COORDINATES OF THE SECOND POINT?\n",
    "def New_Coordinates(lon_o,lat_o,dir_o,dist_o):\n",
    "    R    = 6378.1                           #Radius of the Earth\n",
    "    brng = DEGREES[dir_o==BEARINGS][0]      #Bearing is 90 degrees converted to radians.\n",
    "    d    = float(dist_o)*1.60934                   #Distance in km\n",
    "    lat1 = math.radians(float(lat_o))              #Current lat point converted to radians\n",
    "    lon1 = math.radians(float(lon_o))              #Current long point converted to radians\n",
    "    lat2 = math.asin( math.sin(lat1)*math.cos(d/R)+math.cos(lat1)*math.sin(d/R)*math.cos(brng))\n",
    "    lon2 = lon1 + math.atan2(math.sin(brng)*math.sin(d/R)*math.cos(lat1),math.cos(d/R)-math.sin(lat1)*math.sin(lat2))\n",
    "    lat2 = math.degrees(lat2)\n",
    "    lon2 = math.degrees(lon2)\n",
    "    return(lon2,lat2)\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "#Process\n",
    "BEARINGS    = np.asarray(['N','NNE','NE','ENE','E','ESE','SE','SSE','S','SSW','SW','WSW','W','WNW','NW','NNW'])\n",
    "DEGREES     = np.arange(0,360,45/2)\n",
    "STATES      = np.asarray(['MAINE','NEW HAMPSHIRE','VERMONT','MASSACHUSETTS',\n",
    "                          'RHODE ISLAND','CONNECTICUT','NEW YORK','NEW JERSEY',\n",
    "                          'PENNSYLVANIA','OHIO','WEST VIRGINIA','MARYLAND',\n",
    "                          'DELAWARE','VIRGINIA'])\n",
    "\n",
    "STATIONS    = pd.read_csv('C:/Users/Mike/Desktop/GHCND/PNS_Stations_List.csv')\n",
    "PRINT       = 'PNS_20181131_'\n",
    "IEM_DATA_pd = pd.read_csv('C:/Users/Mike/Desktop/GHCND/IEM_PNS/PNS_20181131.csv', header=None)\n",
    "#Clean IEM pd\n",
    "#(TOWN STATE COUNTY),DISTANCE,BEARING,AMOUNT\n",
    "IEM_DATA_np  = np.empty([len(IEM_DATA_pd),4], dtype=object)\n",
    "for i in range(len(IEM_DATA_pd)):\n",
    "    IEM_DATA_pd_i = IEM_DATA_pd[0][i]\n",
    "    if IEM_DATA_pd_i in STATES:  STATE  = IEM_DATA_pd_i.strip()                     #OBTAIN THE STATE INFORMATION\n",
    "    elif '...' in IEM_DATA_pd_i: COUNTY = IEM_DATA_pd_i.replace('...','').strip()   #OBTAIN THE COUNTY INFORMATION\n",
    "    else:                                                                           #CREATE THE JOIN ARRAY\n",
    "        IEM_DATA_pd_i = IEM_DATA_pd[0][i].strip().split(\" \")                        #IF NO BEARING OR DISTANCE IS GIVEN\n",
    "        if len(re.sub('\\D', '', IEM_DATA_pd_i[0]))==0:\n",
    "            DISTANCE  = 'NONE'\n",
    "            DIRECTION = 'NONE'\n",
    "            TOWN_list = ''\n",
    "            for j in range(len(IEM_DATA_pd_i)):\n",
    "                if (''==IEM_DATA_pd_i[j]) or (len(re.sub('\\D', '', IEM_DATA_pd_i[j]))==0):\n",
    "                    TOWN_list += IEM_DATA_pd_i[j]+' '\n",
    "                else: \n",
    "                    AMNT = IEM_DATA_pd_i[j]\n",
    "                    break \n",
    "            IEM_DATA_np[i,0] = TOWN_list.strip()+' '+STATE+' '+COUNTY\n",
    "            IEM_DATA_np[i,1] = DIRECTION\n",
    "            IEM_DATA_np[i,2] = DISTANCE\n",
    "            IEM_DATA_np[i,3] = AMNT\n",
    "        else:                                                                      #IF BEARING AND DISTANCE IS GIVEN\n",
    "            DISTANCE      = IEM_DATA_pd_i[0]\n",
    "            DIRECTION     = IEM_DATA_pd_i[1]\n",
    "            TOWN_list     = ''\n",
    "            IEM_DATA_pd_i = IEM_DATA_pd_i[2:]\n",
    "            for j in range(len(IEM_DATA_pd_i)):\n",
    "                if (''==IEM_DATA_pd_i[j]) or (len(re.sub('\\D', '', IEM_DATA_pd_i[j]))==0):\n",
    "                    TOWN_list += IEM_DATA_pd_i[j]+' '\n",
    "                else: \n",
    "                    AMNT = IEM_DATA_pd_i[j]\n",
    "                    break \n",
    "            IEM_DATA_np[i,0] = TOWN_list.strip()+' '+STATE+' '+COUNTY\n",
    "            IEM_DATA_np[i,1] = DIRECTION\n",
    "            IEM_DATA_np[i,2] = DISTANCE\n",
    "            IEM_DATA_np[i,3] = AMNT\n",
    "#CLEAN THE MATRIX AND TURN INTO PANDAS DF\n",
    "IEM_DATA_np      = IEM_DATA_np[np.sum(IEM_DATA_np==None,axis=1)!=4]\n",
    "IEM_DATA_pd      = pd.DataFrame(IEM_DATA_np)\n",
    "IEM_DATA_pd.rename(columns={0:'Location',1:'Bearing',2:'Distance',3:'Snowfall_inch'}, inplace=True)\n",
    "#JOIN COORDINATES\n",
    "IEM_Joined_pd    = IEM_DATA_pd.merge(STATIONS, on='Location', how='inner').reset_index(drop=True)\n",
    "#CHECK JOIN\n",
    "Joined_Locations = IEM_Joined_pd['Location']\n",
    "Availa_Locations = IEM_DATA_pd['Location']\n",
    "Logica_Locations = []\n",
    "print(\"There are \"+str(len(Availa_Locations)-len(IEM_Joined_pd))+\" Missing Stations!!!\")\n",
    "for i in range(len(Availa_Locations)):\n",
    "    if Availa_Locations[i] in np.asarray(Joined_Locations): pass\n",
    "    else: print(Availa_Locations[i]) \n",
    "print(\"Copy and paste the above stations to: https://geocode.localfocus.nl/\")\n",
    "print(\"Add to stations list\")\n",
    "#CORRECT FOR BEARING AND DIRECTION\n",
    "for i in range(len(IEM_Joined_pd)):\n",
    "    if IEM_Joined_pd['Bearing'][i] != 'NONE':\n",
    "        lon_f,lat_f = New_Coordinates(IEM_Joined_pd['Lon'][i],IEM_Joined_pd['Lat'][i],\n",
    "                                      IEM_Joined_pd['Bearing'][i],IEM_Joined_pd['Distance'][i])\n",
    "        IEM_Joined_pd['Lat'][i] = str(lat_f)\n",
    "        IEM_Joined_pd['Lon'][i] = str(lon_f)\n",
    "IEM_Joined_pd = IEM_Joined_pd[['Lon','Lat','Snowfall_inch']]\n",
    "IEM_Joined_pd = IEM_Joined_pd.drop_duplicates()\n",
    "IEM_Joined_pd.to_csv(PRINT+\"processed.csv\", sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Krig_hole-effect_20191129_20191203.txt']\n",
      "['Krig_hole-effect_20190118_20190121.txt']\n",
      "['Krig_spherical_0.25_20181114_20181116.txt']\n",
      "['Krig_hole-effect_20180320_20180322.txt']\n",
      "['Krig_hole-effect_20180311_20180315.txt']\n",
      "['Krig_spherical_0.2_20180305_20180308.txt']\n",
      "['Krig_hole-effect_20180301_20180303.txt', 'NOTES.txt']\n",
      "['Krig_spherical_0.2_20180103_20180105.txt']\n",
      "['Krig_hole-effect_20170312_20170315.txt']\n",
      "['Krig_spherical_0.6_20170209_20170210.txt']\n",
      "['Krig_hole-effect_20161117_20161122.txt', 'NOTES.txt']\n",
      "['Krig_hole-effect_20160122_20160124.txt']\n",
      "['Krig_hole-effect_20150214_20150216.txt']\n",
      "['Krig_hole-effect_20150208_20150210.txt']\n",
      "['Krig_spherical_0.25_20150129_20150203.txt']\n",
      "['Krig_hole-effect_20150125_20150128.txt']\n",
      "['Krig_hole-effect_20141209_20141214.txt']\n",
      "['Krig_hole-effect_20141126_20141128.txt']\n",
      "['Krig_hole-effect_20140211_20140214.txt']\n",
      "['Krig_spherical_0.2_20140120_20140122.txt']\n",
      "['Krig_20131230_20140103.txt']\n",
      "['Krig_hole-effect_20131213_20131216.txt']\n",
      "['Krig_spherical_0.25_20130317_20130320.txt']\n",
      "['Krig_hole-effect_0.5_20130303_20130309.txt']\n",
      "['Krig_hole-effect_20130208_20130210.txt']\n",
      "['Krig_spherical_20121228_20121231.txt']\n",
      "['Krig_hole-effect_20121224_20121228.txt']\n",
      "['Krig_hole-effect_20111025_20111031.txt']\n",
      "['Krig_hole-effect_20110224_20110227.txt']\n",
      "['Krig_hole-effect_20110201_20110204.txt']\n",
      "['Krig_hole-effect_20110126_20110127.txt']\n",
      "['Krig_hole-effect_20110109_20110113.txt']\n",
      "['Krig_hole-effect_20101224_20101228.txt']\n",
      "['Krig_hole-effect_20100221_20100301.txt']\n",
      "['Krig_hole-effect_20100212_20100219.txt']\n",
      "['Krig_hole-effect_20100208_20100211.txt']\n",
      "['Krig_hole-effect_20100204_20100208.txt']\n",
      "['Krig_hole-effect_20091228_20100104.txt']\n",
      "['Krig_hole-effect_20091218_20091221.txt']\n",
      "['Krig_spherical_20091207_20091211.txt']\n",
      "['Krig_20090226_20090303.txt']\n",
      "['Krig_20090222_20090224.txt']\n",
      "['Krig_hole-effect_1_20081221_20081223.txt', 'Krig_spherical_0.25_20081221_20081223.txt']\n",
      "['Krig_hole-effect_1_20081218_20081222.txt']\n",
      "['Krig_20071214_20071217.txt']\n",
      "['Krig_20071130_20071204.txt']\n",
      "['Krig_20070403_20070406.txt']\n",
      "['Krig_hole-effect_20070316_20070318.txt']\n",
      "['Krig_hole-effect_20070211_20070216.txt']\n",
      "['Krig_hole-effect_20060210_20060214.txt']\n",
      "['Krig_spherical_0.25_20050228_20050302.txt']\n",
      "['Krig_hole-effect_20050122_20050124.txt']\n",
      "['Krig_hole-effect_20031204_20031208.txt']\n",
      "['Krig_hole-effect_20030214_20030218.txt']\n",
      "['Krig_hole-effect_20030101_20030104.txt']\n",
      "['Krig_hole-effect_20021223_20021226.txt']\n",
      "['Krig_hole-effect_1_20001228_20010101.txt']\n",
      "['Krig_spherical_0.2_20000216_20000220.txt']\n",
      "['Krig_hole-effect_20000124_20000201.txt']\n",
      "['Krig_hole-effect_20000124_20000127.txt']\n",
      "['Krig_19990312_19990316.txt']\n",
      "['Krig_spherical_0.25_19990113_19990116.txt']\n",
      "['Krig_hole-effect_19970331_19970401.txt']\n",
      "['Krig_19970108_19970112.txt']\n",
      "['Krig_spherical_0.25_19960409_19960411.txt']\n",
      "['Krig_hole-effect_19960303_19960309.txt']\n",
      "['Krig_19960201_19960204.txt']\n",
      "['Krig_hole-effect_19960106_19960109.txt']\n",
      "['Krig_spherical_0.2_19951218_19951222.txt']\n",
      "['Krig_hole-effect_19950202_19950206.txt']\n",
      "['Krig_spherical_0.2_19940228_19940304.txt']\n",
      "['Krig_19940222_19940225.txt']\n",
      "['Krig_hole-effect_19940116_19940118.txt']\n",
      "['Krig_hole-effect_19940104_19940109.txt']\n",
      "['Krig_hole-effect_19940101_19940105.txt']\n",
      "['Krig_hole-effect_19930312_19930315.txt']\n",
      "['Krig_19930220_19930224.txt']\n",
      "['Krig_19930214_19930218.txt']\n",
      "['Krig_hole-effect_19921209_19921213.txt']\n",
      "['Krig_hole-effect_19901227_19901229.txt']\n",
      "['Krig_19880209_19880214.txt']\n",
      "['Krig_19880122_19880127.txt']\n",
      "['Krig_hole-effect_19880105_19880109.txt']\n",
      "['Krig_19871213_19871217.txt']\n",
      "['Krig_hole-effect_19871110_19871112.txt']\n",
      "['Krig_19870222_19870224.txt']\n",
      "['Krig_hole-effect_19870121_19870124.txt']\n",
      "['Krig_19870108_19870112.txt']\n",
      "['Krig_hole-effect_19870101_19870103.txt']\n",
      "['Krig_19850301_19850305.txt']\n",
      "['Krig_hole-effect_19850129_19850203.txt']\n",
      "['Krig_hole-effect_19840224_19840301.txt']\n",
      "['Krig_spherical_19830210_19830213.txt']\n",
      "['Krig_spherical_0.2_19820404_19820408.txt']\n",
      "['Krig_hole-effect_19820120_19820124.txt']\n",
      "['Krig_19820112_19820115.txt']\n",
      "['Krig_hole-effect_19790217_19790220.txt']\n",
      "['Krig_hole-effect_19790206_19790209.txt']\n",
      "['Krig_hole-effect_19780204_19780208.txt']\n",
      "['Krig_hole-effect_19780117_19780121.txt']\n",
      "['Krig_19780114_19780119.txt']\n",
      "['Krig_hole-effect_19780111_19780115.txt']\n",
      "['Krig_hole-effect_19770321_19770325.txt']\n",
      "['Krig_spherical_19770107_19770111.txt']\n",
      "['Krig_spherical_19740108_19740112.txt']\n",
      "['Krig_hole-effect_19731215_19731218.txt']\n",
      "['Krig_hole-effect_19720216_19720220.txt']\n",
      "['Krig_hole-effect_19711123_19711127.txt']\n",
      "['Krig_hole-effect_19710226_19710306.txt']\n",
      "['Krig_hole-effect_19701231_19710102.txt']\n",
      "['Krig_hole-effect_19691225_19691229.txt']\n",
      "['Krig_hole-effect_19690225_19690304.txt']\n",
      "['Krig_hole-effect_19690222_19690228.txt']\n",
      "['Krig_hole-effect_19690208_19690210.txt']\n",
      "['Krig_19670320_19670323.txt']\n",
      "['Krig_hole-effect_19670206_19670208.txt']\n",
      "['Krig_19661222_19661226.txt']\n",
      "['Krig_19660223_19660226.txt']\n",
      "['Krig_19660128_19660201.txt']\n",
      "['Krig_spherical_0.3_19660121_19660124.txt']\n",
      "['Krig_hole-effect_19640218_19640221.txt']\n",
      "['Krig_hole-effect_19640109_19640114.txt']\n",
      "['Krig_19631221_19631224.txt']\n",
      "['Krig_hole-effect_19620305_19620309.txt']\n",
      "['Krig_hole-effect_19620228_19620307.txt']\n",
      "['Krig_spherical_0.25_19620213_19620216.txt']\n",
      "['Krig_hole-effect_19610201_19610205.txt']\n",
      "['Krig_hole-effect_19610118_19610121.txt']\n",
      "['Krig_hole-effect_19601210_19601213.txt']\n",
      "['Krig_spherical_0.2_19600229_19600305.txt']\n",
      "['Krig_19600212_19600215.txt']\n",
      "['Krig_spherical_19590312_19590314.txt']\n",
      "['Krig_hole-effect_19580318_19580323.txt']\n",
      "['Krig_spherical_0.45_19580212_19580218.txt']\n",
      "['Krig_hole-effect_19571203_19571205.txt']\n",
      "['Krig_hole-effect_19560318_19560320.txt']\n",
      "['Krig_hole-effect_19560314_19560317.txt']\n",
      "['Krig_hole-effect_19560303_19560309.txt']\n",
      "['Krig_hole-effect_19520217_19520218.txt']\n",
      "['Krig_hole-effect_19511213_19511216.txt']\n",
      "['Krig_hole-effect_19501122_19501130.txt']\n",
      "['Krig_spherical_0.2_19500211_19500217.txt', 'Krig_spherical_19500211_19500217.txt']\n",
      "['Krig_19490129_19490201.txt']\n",
      "['Krig_19480123_19480125.txt']\n",
      "['Krig_hole-effect_19471225_19471227.txt']\n",
      "['Krig_hole-effect_19470227_19470304.txt']\n",
      "['Krig_spherical_19470220_19470224.txt']\n",
      "['Krig_hole-effect_19460215_19460221.txt']\n",
      "['Krig_spherical_0.3_19451217_19451220.txt']\n",
      "['Krig_spherical_0.2_19450113_19450117.txt']\n",
      "['Krig_hole-effect_19441208_19441213.txt']\n",
      "['Krig_spherical_0.2_19440208_19440213.txt']\n",
      "['Krig_hole-effect_19430125_19430129.txt']\n",
      "['Krig_spherical_0.2_19420328_19420331.txt']\n",
      "['Krig_hole-effect_19420328_19420331.txt']\n",
      "['Krig_spherical_0.2_19410307_19410310.txt']\n",
      "['Krig_hole-effect_19400213_19400215.txt']\n",
      "['Krig_spherical_0.5_19381123_19381125.txt']\n",
      "['Krig_spherical_0.2_19360210_19360215.txt']\n",
      "['Krig_hole-effect_19360118_19360121.txt']\n",
      "['Krig_hole-effect_19350121_19350125.txt']\n",
      "['Krig_spherical_0.2_19340223_19340227.txt']\n",
      "['Krig_spherical_0.5_19331226_19331227.txt']\n",
      "['Krig_19321216_19321218.txt']\n",
      "['Krig_hole-effect_19310304_19310312.txt']\n",
      "['Krig_19291219_19291224.txt']\n",
      "['Krig_19290220_19290222.txt']\n",
      "['Krig_hole-effect_19270216_19270221.txt']\n",
      "['Krig_hole-effect_19260203_19260205.txt']\n",
      "['Krig_hole-effect_19260107_19260110.txt']\n",
      "['Krig_hole-effect_19250128_19250131.txt']\n",
      "['Krig_hole-effect_0.5_19241231_19250103.txt']\n",
      "['Krig_19240401_19240404.txt']\n",
      "['Krig_spherical_19240217_19240221.txt']\n",
      "['Krig_spherical_1_19230203_19230207.txt']\n",
      "['Krig_spherical_0.3_19220126_19220130.txt']\n",
      "['Krig_19210218_19210222.txt']\n",
      "['Krig_hole-effect_19200204_19200207.txt']\n",
      "['Krig_19180125_19180129.txt']\n",
      "['Krig_hole-effect_19180121_19180123.txt']\n",
      "['Krig_hole-effect_19180112_19180116.txt']\n",
      "['Krig_spherical_0.5_19171212_19171215.txt']\n",
      "['Krig_spherical_19171206_19171209.txt']\n",
      "['Krig_hole-effect_19170302_19170306.txt']\n",
      "['Krig_hole-effect_19160302_19160309.txt']\n",
      "['Krig_hole-effect_19151210_19151215.txt']\n",
      "['Krig_hole-effect_19150403_19150405.txt']\n",
      "['Krig_hole-effect_19150302_19150308.txt']\n",
      "['Krig_19150129_19150203.txt']\n",
      "['Krig_hole-effect_19140212_19140215.txt']\n",
      "['Krig_19100216_19100218.txt']\n",
      "['Krig_hole-effect_19100210_19100213.txt']\n",
      "['Krig_19100112_19100115.txt']\n",
      "['Krig_hole-effect_19091223_19091227.txt']\n",
      "['Krig_hole-effect_19090302_19090305.txt']\n",
      "[]\n",
      "['Krig_hole-effect_19090110_19090115.txt']\n",
      "['Krig_19080216_19080220.txt']\n",
      "['Krig_19080203_19080207.txt']\n",
      "['Krig_spherical_19080129_19080202.txt']\n",
      "['Krig_hole-effect_19070204_19070206.txt']\n",
      "['Krig_19060317_19060320.txt']\n",
      "['Krig_hole-effect_19060312_19060317.txt']\n",
      "['Krig_19040127_19040130.txt']\n",
      "['Krig_hole-effect_19030214_19030218.txt']\n",
      "['Krig_hole-effect_19021211_19021214.txt']\n",
      "['Krig_19020303_19020306.txt']\n",
      "['Krig_hole-effect_19020213_19020219.txt']\n",
      "['Krig_19010201_19010206.txt']\n",
      "['Krig_19000315_19000316.txt']\n",
      "['Krig_hole-effect_19000226_19000303.txt']\n"
     ]
    }
   ],
   "source": [
    "LIST_DIR = pd.read_csv('C:/Users/Mike/NESIS_Kriging/Final_Interpolations/Listing.csv')\n",
    "TEXT_DIR = 'C:/Users/Mike/NESIS_Kriging/Final_Interpolations/grid_txt/'\n",
    "for i in range(len(LIST_DIR)):\n",
    "    DIR_LISTING  = TEXT_DIR+LIST_DIR['Folder'][i]\n",
    "    TEXT_LISTING = find_files( DIR_LISTING, suffix=\".txt\" )\n",
    "    #print(TEXT_LISTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
